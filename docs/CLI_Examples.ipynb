{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ–¥ï¸ CLI Examples - PTOF Analysis System\n",
    "\n",
    "Questo notebook contiene tutti gli snippet di codice per utilizzare gli script CLI del sistema di analisi PTOF.\n",
    "\n",
    "**âš™ï¸ Configurazione**: Tutti i comandi utilizzano il virtual environment `.venv` del progetto.\n",
    "\n",
    "**ğŸ“‚ Struttura Directory**:\n",
    "- `ptof_inbox/` â†’ PDF da analizzare (INSERISCI QUI)\n",
    "- `ptof_processed/` â†’ PDF archiviati automaticamente\n",
    "\n",
    "## ğŸ“‘ Indice\n",
    "1. [ğŸš€ **Workflow Automatico**](#workflow-automatico) â­ PRIORITÃ€\n",
    "2. [ğŸ”— Analisi Multi-Agente Batch](#multi-agent-batch)\n",
    "3. [â˜ï¸ Cloud Agent](#cloud-agent)\n",
    "4. [ğŸ” Analisi e Revisione](#analysis-review)\n",
    "5. [ğŸ› ï¸ Utility](#utilities)\n",
    "6. [ğŸ¤– Automazione Background](#background-automation)\n",
    "7. [ğŸ“Š Diagnostica](#diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Workflow Automatico {#workflow-automatico}\n",
    "\n",
    "### â­ Pipeline Completa: Inbox â†’ Processed\n",
    "\n",
    "**Cosa fa**:\n",
    "1. ğŸ“¥ Legge PDF da `ptof_inbox/`\n",
    "2. ğŸ“ Converte PDF â†’ Markdown\n",
    "3. ğŸ¤– Analizza con pipeline multi-agente\n",
    "4. ğŸ“¦ Sposta PDF in `ptof_processed/batch_TIMESTAMP/`\n",
    "5. ğŸ“Š Aggiorna CSV dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Workflow Completo: Validazione + Analisi (One-Click)\n",
    "\n",
    "Questa Ã¨ la cella **principale** per processare nuovi PTOF. Esegue l'intera pipeline automaticamente.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "La cella usa configurazioni di default ottimizzate. Modifica queste variabili se necessario:\n",
    "\n",
    "| Parametro | Posizione | Descrizione | Default |\n",
    "|-----------|-----------|-------------|---------|\n",
    "| `use_llm_if_ambiguous` | Step 0 | Usa Ollama per file ambigui | `True` |\n",
    "| Modello LLM | `agentic_pipeline.py` | Modello per analisi | `gemma3:27b` |\n",
    "| Modello Validazione | `ptof_validator.py` | Modello per validazione | `qwen3:32b` |\n",
    "\n",
    "### ğŸ“‹ Pipeline Completa\n",
    "\n",
    "| Step | Descrizione | Output |\n",
    "|------|-------------|--------|\n",
    "| 0 | **Pre-Validazione** (heuristics â†’ LLM) | File non-PTOF â†’ `ptof_discarded/` |\n",
    "| 1 | **Conversione PDF â†’ Markdown** | `ptof_md/*.md` |\n",
    "| 2 | **Analisi Multi-Agente** (Analyst â†’ Reviewer â†’ Refiner) | `analysis_results/*.json` |\n",
    "| 3 | **Archiviazione PDF** | `ptof_processed/batch_*/` |\n",
    "| 4 | **Rebuild CSV** | `data/analysis_summary.csv` |\n",
    "\n",
    "### ğŸ“‚ Directory\n",
    "\n",
    "| Input | Output |\n",
    "|-------|--------|\n",
    "| `ptof_inbox/*.pdf` | `analysis_results/*.json` |\n",
    "| | `ptof_md/*.md` |\n",
    "| | `data/analysis_summary.csv` |\n",
    "\n",
    "### âš ï¸ Note Importanti\n",
    "\n",
    "- **Tempo**: ~2-5 minuti per PTOF (dipende dalla lunghezza)\n",
    "- **Ollama**: Deve essere in esecuzione (`ollama serve`)\n",
    "- **Memoria**: Consigliati 16GB+ RAM per modelli 27B/32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 4: WORKFLOW COMPLETO (VALIDAZIONE + ANALISI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ WORKFLOW COMPLETO: VALIDAZIONE + ANALISI (ONE-CLICK)\n",
    "# Esegui questa cella per processare automaticamente tutti i PDF in ptof_inbox/\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurazione\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "os.chdir(BASE_DIR)\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "INBOX_DIR = BASE_DIR / \"ptof_inbox\"\n",
    "PROCESSED_DIR = BASE_DIR / \"ptof_processed\"\n",
    "MD_DIR = BASE_DIR / \"ptof_md\"\n",
    "ANALYSIS_DIR = BASE_DIR / \"analysis_results\"\n",
    "CSV_FILE = BASE_DIR / \"data\" / \"analysis_summary.csv\"\n",
    "\n",
    "# Crea directory\n",
    "for d in [INBOX_DIR, PROCESSED_DIR, MD_DIR, ANALYSIS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ WORKFLOW COMPLETO: VALIDAZIONE + ANALISI\")\n",
    "print(f\"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =====================================================\n",
    "# STEP 0: PRE-VALIDAZIONE\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” STEP 0: Pre-Validazione PTOF (heuristics â†’ LLM)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from src.validation.ptof_validator import PTOFValidator, ValidationResult\n",
    "\n",
    "validator = PTOFValidator()\n",
    "inbox_pdfs = list(INBOX_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "print(f\"ğŸ“¥ PDF in inbox: {len(inbox_pdfs)}\")\n",
    "\n",
    "if not inbox_pdfs:\n",
    "    print(\"âš ï¸ Nessun PDF da processare!\")\n",
    "    print(\"ğŸ’¡ Copia i PDF in ptof_inbox/ e riprova\")\n",
    "else:\n",
    "    # Valida tutti i PDF\n",
    "    valid_pdfs = []\n",
    "    discarded_count = 0\n",
    "    \n",
    "    for i, pdf_path in enumerate(inbox_pdfs, 1):\n",
    "        print(f\"\\n[{i}/{len(inbox_pdfs)}] ğŸ“„ {pdf_path.name}\")\n",
    "        \n",
    "        report = validator.validate(pdf_path, use_llm_if_ambiguous=True)\n",
    "        \n",
    "        if report.result == ValidationResult.VALID_PTOF.value:\n",
    "            print(f\"   âœ… VALIDO (confidence: {report.confidence:.2f}, fase: {report.phase})\")\n",
    "            valid_pdfs.append(pdf_path)\n",
    "        else:\n",
    "            print(f\"   âŒ SCARTATO: {report.reason}\")\n",
    "            validator.discard(pdf_path, report)\n",
    "            discarded_count += 1\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Validazione: {len(valid_pdfs)} validi, {discarded_count} scartati\")\n",
    "    \n",
    "    if not valid_pdfs:\n",
    "        print(\"\\nâš ï¸ Nessun PTOF valido trovato!\")\n",
    "        print(\"ğŸ’¡ Per recuperare file scartati: usa la cella 'Recovery'\")\n",
    "    else:\n",
    "        # =====================================================\n",
    "        # STEP 1: CONVERSIONE PDF â†’ MARKDOWN\n",
    "        # =====================================================\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“ STEP 1: Conversione PDF â†’ Markdown\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        from src.processing.convert_pdfs_to_md import pdf_to_markdown\n",
    "        \n",
    "        converted = []\n",
    "        code_mapping = {}\n",
    "        \n",
    "        for pdf_path in valid_pdfs:\n",
    "            # Estrai codice dal filename\n",
    "            basename = pdf_path.stem.upper()\n",
    "            match = re.match(r'^([A-Z]{2}[A-Z]{2}[A-Z0-9]{6})', basename)\n",
    "            if match:\n",
    "                school_code = match.group(1)\n",
    "            else:\n",
    "                parts = re.split(r'[_\\-\\s]', basename)\n",
    "                school_code = parts[0][:10] if parts else basename[:10]\n",
    "            \n",
    "            md_output = MD_DIR / f\"{school_code}.md\"\n",
    "            \n",
    "            # Verifica duplicato\n",
    "            analysis_file = ANALYSIS_DIR / f\"{school_code}_analysis.json\"\n",
    "            if analysis_file.exists():\n",
    "                print(f\"â­ï¸ GiÃ  analizzato: {school_code}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"ğŸ”„ Convertendo: {pdf_path.name} â†’ {school_code}.md\")\n",
    "            \n",
    "            try:\n",
    "                if pdf_to_markdown(str(pdf_path), str(md_output)):\n",
    "                    converted.append(pdf_path)\n",
    "                    code_mapping[str(pdf_path)] = school_code\n",
    "                    print(f\"   âœ… Convertito!\")\n",
    "                else:\n",
    "                    print(f\"   âŒ Errore conversione\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Errore: {e}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Convertiti: {len(converted)} file\")\n",
    "        \n",
    "        if converted:\n",
    "            # =====================================================\n",
    "            # STEP 2: ANALISI MULTI-AGENTE\n",
    "            # =====================================================\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ğŸ¤– STEP 2: Analisi Multi-Agente\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            from app.agentic_pipeline import (\n",
    "                AnalystAgent, ReviewerAgent, RefinerAgent, SynthesizerAgent,\n",
    "                process_single_ptof\n",
    "            )\n",
    "            \n",
    "            # Inizializza agenti\n",
    "            print(\"ğŸ”§ Inizializzazione agenti...\")\n",
    "            analyst = AnalystAgent()\n",
    "            reviewer = ReviewerAgent()\n",
    "            refiner = RefinerAgent()\n",
    "            synthesizer = SynthesizerAgent()\n",
    "            print(\"   âœ… Agenti pronti!\")\n",
    "            \n",
    "            analyzed = []\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            for pdf_path in converted:\n",
    "                school_code = code_mapping.get(str(pdf_path))\n",
    "                md_file = MD_DIR / f\"{school_code}.md\"\n",
    "                \n",
    "                if not md_file.exists():\n",
    "                    print(f\"âš ï¸ MD non trovato: {school_code}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nğŸ”„ Analizzando: {school_code}\")\n",
    "                \n",
    "                def status_callback(msg):\n",
    "                    # Evidenzia log agenti\n",
    "                    if 'Analyst' in msg or 'Reviewer' in msg or 'Refiner' in msg:\n",
    "                        print(f\"   ğŸ¤– {msg}\")\n",
    "                    elif 'chunk' in msg.lower():\n",
    "                        print(f\"   ğŸ“¦ {msg}\")\n",
    "                \n",
    "                try:\n",
    "                    result = process_single_ptof(\n",
    "                        md_file=str(md_file),\n",
    "                        analyst=analyst,\n",
    "                        reviewer=reviewer,\n",
    "                        refiner=refiner,\n",
    "                        synthesizer=synthesizer,\n",
    "                        status_callback=status_callback\n",
    "                    )\n",
    "                    \n",
    "                    if result:\n",
    "                        analyzed.append(school_code)\n",
    "                        meta = result.get('metadata', {})\n",
    "                        print(f\"   âœ… Completato: {meta.get('denominazione', school_code)[:50]}\")\n",
    "                        \n",
    "                        # Aggiorna CSV dopo ogni analisi\n",
    "                        subprocess.run(\n",
    "                            ['python', 'src/processing/rebuild_csv.py'],\n",
    "                            capture_output=True, timeout=60\n",
    "                        )\n",
    "                    else:\n",
    "                        print(f\"   âš ï¸ Nessun risultato\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Errore: {e}\")\n",
    "            \n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            print(f\"\\nğŸ“Š Analizzati: {len(analyzed)} file in {elapsed:.1f}s\")\n",
    "            \n",
    "            # =====================================================\n",
    "            # STEP 3: ARCHIVIAZIONE\n",
    "            # =====================================================\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ğŸ“¦ STEP 3: Archiviazione PDF\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            batch_dir = PROCESSED_DIR / f\"batch_{timestamp}\"\n",
    "            batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for pdf_path in converted:\n",
    "                try:\n",
    "                    dest = batch_dir / pdf_path.name\n",
    "                    shutil.move(str(pdf_path), str(dest))\n",
    "                    print(f\"   âœ… {pdf_path.name} â†’ processed/\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Errore: {e}\")\n",
    "            \n",
    "            # =====================================================\n",
    "            # STEP 4: REBUILD CSV FINALE\n",
    "            # =====================================================\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ğŸ“Š STEP 4: Aggiornamento CSV Dashboard\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                ['python', 'src/processing/rebuild_csv.py'],\n",
    "                capture_output=True, text=True, timeout=120\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                import pandas as pd\n",
    "                if CSV_FILE.exists():\n",
    "                    df = pd.read_csv(CSV_FILE)\n",
    "                    print(f\"   âœ… CSV aggiornato: {len(df)} scuole totali\")\n",
    "            else:\n",
    "                print(f\"   âŒ Errore: {result.stderr}\")\n",
    "\n",
    "# =====================================================\n",
    "# RIEPILOGO FINALE\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š RIEPILOGO FINALE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Conta file\n",
    "inbox_count = len(list(INBOX_DIR.glob(\"*.pdf\")))\n",
    "processed_count = len(list(PROCESSED_DIR.rglob(\"*.pdf\")))\n",
    "md_count = len(list(MD_DIR.glob(\"*.md\")))\n",
    "analysis_count = len(list(ANALYSIS_DIR.glob(\"*.json\")))\n",
    "discarded_list = validator.list_discarded()\n",
    "\n",
    "print(f\"   ğŸ“¥ PDF in inbox: {inbox_count}\")\n",
    "print(f\"   âœ… PDF processati: {processed_count}\")\n",
    "print(f\"   ğŸ“ File Markdown: {md_count}\")\n",
    "print(f\"   ğŸ“Š Analisi JSON: {analysis_count}\")\n",
    "print(f\"   ğŸ—‘ï¸ File scartati: {len(discarded_list)}\")\n",
    "\n",
    "if discarded_list:\n",
    "    print(f\"\\nğŸ’¡ Per recuperare file scartati:\")\n",
    "    print(f\"   â†’ Esegui la cella 'Recovery' sotto\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… WORKFLOW COMPLETATO!\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”„ Ricarica la dashboard per vedere i nuovi dati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Pre-Validazione PTOF (Solo Validazione)\n",
    "\n",
    "Questa cella esegue **solo la validazione** dei PDF senza analisi. Usa questa cella per:\n",
    "- Controllare quali PDF sono veri PTOF prima di analizzarli\n",
    "- Pulire la cartella inbox da documenti non pertinenti\n",
    "- Testare il validatore\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Descrizione | Valore Default |\n",
    "|-----------|-------------|----------------|\n",
    "| `MOVE_INVALID` | Sposta file non validi in `ptof_discarded/` | `True` |\n",
    "\n",
    "### ğŸ“‹ Validazione Progressiva\n",
    "\n",
    "| Fase | Metodo | VelocitÃ  | Quando usato |\n",
    "|------|--------|----------|--------------|\n",
    "| 1 | **Heuristics** | âš¡ Veloce | Sempre (keywords, struttura, pagine) |\n",
    "| 2 | **Ollama LLM** | ğŸ¢ Lento | Solo se ambiguo (confidence 0.2-0.7) |\n",
    "\n",
    "### ğŸ“‚ Directory\n",
    "\n",
    "| Cartella | Contenuto |\n",
    "|----------|-----------|\n",
    "| `ptof_inbox/` | PDF da validare |\n",
    "| `ptof_discarded/not_ptof/` | Circolari, moduli, verbali... |\n",
    "| `ptof_discarded/too_short/` | Documenti < 5 pagine |\n",
    "| `ptof_discarded/corrupted/` | PDF illeggibili |\n",
    "\n",
    "### âš ï¸ Nota\n",
    "\n",
    "Per workflow completo (validazione + analisi), usa la cella \"Workflow Completo\" sopra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 6: PRE-VALIDAZIONE PTOF\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” PRE-VALIDAZIONE PTOF - Identifica e scarta documenti non-PTOF\n",
    "# Validazione progressiva: Heuristics (veloce) â†’ Ollama (se ambiguo)\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurazione\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "os.chdir(BASE_DIR)\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "PTOF_INBOX_DIR = BASE_DIR / \"ptof_inbox\"\n",
    "DISCARDED_DIR = BASE_DIR / \"ptof_discarded\"\n",
    "\n",
    "# =====================================================\n",
    "# INIZIALIZZA VALIDATORE\n",
    "# =====================================================\n",
    "from src.validation.ptof_validator import PTOFValidator, ValidationResult\n",
    "\n",
    "validator = PTOFValidator()\n",
    "\n",
    "# =====================================================\n",
    "# 1. MOSTRA STATO ATTUALE\n",
    "# =====================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” PRE-VALIDAZIONE PTOF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Conta PDF\n",
    "inbox_pdfs = list(PTOF_INBOX_DIR.glob(\"*.pdf\"))\n",
    "discarded = validator.list_discarded()\n",
    "\n",
    "print(f\"\\nğŸ“Š STATO ATTUALE:\")\n",
    "print(f\"   ğŸ“¥ PDF in inbox: {len(inbox_pdfs)}\")\n",
    "print(f\"   ğŸ—‘ï¸ File scartati: {len(discarded)}\")\n",
    "\n",
    "if inbox_pdfs:\n",
    "    print(f\"\\nğŸ“„ PDF in inbox:\")\n",
    "    for i, pdf in enumerate(inbox_pdfs[:15], 1):\n",
    "        print(f\"   {i}. {pdf.name}\")\n",
    "    if len(inbox_pdfs) > 15:\n",
    "        print(f\"   ... e altri {len(inbox_pdfs) - 15}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. VALIDAZIONE BATCH\n",
    "# =====================================================\n",
    "if inbox_pdfs:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ” AVVIO VALIDAZIONE PROGRESSIVA\")\n",
    "    print(\"   Fase 1: Heuristics (keywords, struttura)\")\n",
    "    print(\"   Fase 2: Ollama (se ambiguo)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Imposta True per spostare i file non validi\n",
    "    MOVE_INVALID = True  # Cambia in False per simulazione\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    results = validator.validate_batch(PTOF_INBOX_DIR, move_invalid=MOVE_INVALID)\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š RISULTATI VALIDAZIONE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   âœ… PTOF validi: {results['stats']['valid']}\")\n",
    "    print(f\"   âŒ Non-PTOF: {results['stats']['not_ptof']}\")\n",
    "    print(f\"   ğŸ“„ Troppo corti: {results['stats']['too_short']}\")\n",
    "    print(f\"   ğŸ’” Corrotti: {results['stats']['corrupted']}\")\n",
    "    print(f\"   â“ Ambigui: {results['stats']['ambiguous']}\")\n",
    "    print(f\"   â±ï¸ Tempo: {elapsed:.1f}s\")\n",
    "    \n",
    "    if results['not_ptof'] or results['too_short'] or results['corrupted']:\n",
    "        print(f\"\\nğŸ“‚ File scartati spostati in: ptof_discarded/\")\n",
    "        print(\"   ğŸ’¡ Per recuperare: usa la sezione 'Recovery' sotto\")\n",
    "else:\n",
    "    print(\"\\nâœ… Nessun PDF da validare in ptof_inbox/\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. MOSTRA FILE SCARTATI\n",
    "# =====================================================\n",
    "discarded = validator.list_discarded()\n",
    "if discarded:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ—‘ï¸ FILE SCARTATI RECUPERABILI ({len(discarded)})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for item in discarded:\n",
    "        icon = {\"not_ptof\": \"âŒ\", \"too_short\": \"ğŸ“„\", \"corrupted\": \"ğŸ’”\"}.get(item['category'], \"â“\")\n",
    "        print(f\"   {icon} [{item['category']}] {item['name']}\")\n",
    "        if item.get('report'):\n",
    "            print(f\"      â””â”€ {item['report'].get('reason', 'N/A')}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)print(\"=\"*70)\n",
    "print(\"âœ… Validazione completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â™»ï¸ Recovery: Recupera file scartati\n",
    "\n",
    "Questa cella permette di **recuperare file scartati erroneamente** dalla validazione.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Descrizione | Valore Default |\n",
    "|-----------|-------------|----------------|\n",
    "| `FILENAME_TO_RECOVER` | Nome del file da recuperare | `None` (recupera tutti) |\n",
    "| `RECOVER_ALL` | Recupera tutti i file scartati | `False` |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. **Lista file scartati**: Mostra tutti i file in `ptof_discarded/` con motivo dello scarto\n",
    "2. **Recupero singolo**: Imposta `FILENAME_TO_RECOVER = \"nomefile.pdf\"` per recuperare un file specifico\n",
    "3. **Recupero totale**: Imposta `RECOVER_ALL = True` per recuperare tutti i file\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Dopo aver verificato che un file scartato Ã¨ effettivamente un PTOF\n",
    "- Se la validazione LLM ha dato un falso negativo\n",
    "- Per ri-processare file dopo aver corretto il validatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 8: RECOVERY (RECUPERA FILE SCARTATI)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# â™»ï¸ RECOVERY - Recupera file scartati erroneamente\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "os.chdir(BASE_DIR)\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "from src.validation.ptof_validator import PTOFValidator\n",
    "\n",
    "validator = PTOFValidator()\n",
    "\n",
    "# =====================================================\n",
    "# 1. LISTA FILE SCARTATI\n",
    "# =====================================================\n",
    "discarded = validator.list_discarded()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"ğŸ—‘ï¸ FILE SCARTATI RECUPERABILI ({len(discarded)})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not discarded:\n",
    "    print(\"\\nâœ… Nessun file da recuperare\")\n",
    "else:\n",
    "    for i, item in enumerate(discarded, 1):\n",
    "        icon = {\"not_ptof\": \"âŒ\", \"too_short\": \"ğŸ“„\", \"corrupted\": \"ğŸ’”\"}.get(item['category'], \"â“\")\n",
    "        confidence = item.get('report', {}).get('confidence', 0) if item.get('report') else 0\n",
    "        reason = item.get('report', {}).get('reason', 'N/A') if item.get('report') else 'N/A'\n",
    "        \n",
    "        print(f\"\\n[{i}] {icon} {item['name']}\")\n",
    "        print(f\"    Categoria: {item['category']}\")\n",
    "        print(f\"    Motivo: {reason}\")\n",
    "        print(f\"    Confidence: {confidence:.2f}\")\n",
    "        print(f\"    Path: {item['path']}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. RECUPERA UN FILE SPECIFICO\n",
    "# =====================================================\n",
    "# âš ï¸ MODIFICA il nome del file da recuperare\n",
    "\n",
    "FILE_DA_RECUPERARE = None  # Es: \"MIIS08900V_PTOF.pdf\"\n",
    "\n",
    "if FILE_DA_RECUPERARE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"â™»ï¸ RECUPERO: {FILE_DA_RECUPERARE}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Cerca il file\n",
    "    for item in discarded:\n",
    "        if item['name'] == FILE_DA_RECUPERARE:\n",
    "            result = validator.recover(Path(item['path']))\n",
    "            if result:\n",
    "                print(f\"âœ… Recuperato in: {result}\")\n",
    "            else:\n",
    "                print(f\"âŒ Errore recupero\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"âŒ File non trovato: {FILE_DA_RECUPERARE}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. RECUPERA TUTTI I FILE DI UNA CATEGORIA\n",
    "# =====================================================\n",
    "# âš ï¸ Decommenta una delle righe seguenti per recuperare in batch\n",
    "\n",
    "# CATEGORIA_DA_RECUPERARE = \"not_ptof\"   # Documenti non-PTOF\n",
    "# CATEGORIA_DA_RECUPERARE = \"too_short\"  # Troppo corti\n",
    "# CATEGORIA_DA_RECUPERARE = \"corrupted\"  # Corrotti\n",
    "CATEGORIA_DA_RECUPERARE = None\n",
    "\n",
    "if CATEGORIA_DA_RECUPERARE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"â™»ï¸ RECUPERO BATCH: categoria '{CATEGORIA_DA_RECUPERARE}'\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    recovered = validator.recover_all(category=CATEGORIA_DA_RECUPERARE)\n",
    "    print(f\"âœ… Recuperati {len(recovered)} file\")\n",
    "    for path in recovered:\n",
    "        print(f\"   â†’ {path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¡ Per recuperare:\")\n",
    "print(\"   1. Imposta FILE_DA_RECUPERARE = 'nome_file.pdf'\")\n",
    "print(\"   2. Oppure CATEGORIA_DA_RECUPERARE = 'not_ptof'\")\n",
    "print(\"=\"*70)\n",
    "print(\"   3. Ri-esegui la cella\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Diagnostica CSV â†” JSON â†” Database MIUR\n",
    "\n",
    "Questa cella esegue una **diagnosi completa** delle discrepanze tra:\n",
    "- File JSON delle analisi (`analysis_results/*.json`)\n",
    "- CSV della dashboard (`data/analysis_summary.csv`)\n",
    "- Database MIUR (anagrafica scuole ufficiale)\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Descrizione | Valore Default |\n",
    "|-----------|-------------|----------------|\n",
    "| `FIX_JSON` | Corregge automaticamente i JSON con dati MIUR | `True` |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. **Confronta JSON vs MIUR**: Verifica che comune, regione, provincia nei JSON corrispondano ai dati ufficiali MIUR\n",
    "2. **Analizza CSV**: Conta righe e mostra anteprima\n",
    "3. **Correzione automatica**: Se `FIX_JSON=True`, aggiorna i JSON con dati MIUR corretti\n",
    "4. **Rigenera CSV**: Dopo le correzioni, rigenera il CSV per la dashboard\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Dopo aver notato discrepanze nella dashboard\n",
    "- Dopo un batch di analisi per verificare coerenza\n",
    "- Per correggere dati geografici errati estratti dai PTOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª Test Mode: Seleziona PDF per Test\n",
    "\n",
    "Questa cella permette di testare il workflow su un sottoinsieme di PDF.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Descrizione | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `NUM_TEST_FILES` | Numero di PDF da testare | `10` |\n",
    "| `MODE` | `\"setup\"` = prepara test, `\"restore\"` = ripristina tutti | `\"setup\"` |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "**ModalitÃ  `setup`:**\n",
    "1. Sposta tutti i PDF in `ptof_inbox_backup/`\n",
    "2. Copia i N PDF piÃ¹ piccoli in `ptof_inbox/`\n",
    "3. Mostra i file selezionati\n",
    "\n",
    "**ModalitÃ  `restore`:**\n",
    "1. Ripristina tutti i PDF da backup\n",
    "2. Elimina la cartella di backup\n",
    "\n",
    "### ğŸš€ Workflow consigliato\n",
    "\n",
    "1. Esegui con `MODE = \"setup\"` per preparare il test\n",
    "2. Esegui la cella \"Workflow Solo Analisi\" (sotto)\n",
    "3. Verifica i risultati\n",
    "4. Esegui con `MODE = \"restore\"` per ripristinare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ§ª TEST MODE: SETUP\n",
      "======================================================================\n",
      "âš ï¸ Backup giÃ  esistente!\n",
      "   PDF in backup: 77\n",
      "   PDF in inbox: 10\n",
      "\n",
      "ğŸ’¡ Usa MODE = 'restore' per ripristinare prima di un nuovo test\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 11: TEST MODE (SELEZIONA PDF PER TEST)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§ª TEST MODE: Seleziona PDF per test rapido\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# âš™ï¸ CONFIGURAZIONE\n",
    "# =====================================================\n",
    "MODE = \"setup\"  # \"setup\" = prepara test, \"restore\" = ripristina tutti\n",
    "NUM_TEST_FILES = 10  # Numero di PDF da testare (seleziona i piÃ¹ piccoli)\n",
    "\n",
    "# =====================================================\n",
    "# DIRECTORY\n",
    "# =====================================================\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "INBOX_DIR = BASE_DIR / \"ptof_inbox\"\n",
    "BACKUP_DIR = BASE_DIR / \"ptof_inbox_backup\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"ğŸ§ª TEST MODE: {MODE.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if MODE == \"setup\":\n",
    "    # =====================================================\n",
    "    # SETUP: Prepara ambiente di test\n",
    "    # =====================================================\n",
    "    \n",
    "    # Controlla se il backup esiste giÃ \n",
    "    if BACKUP_DIR.exists() and list(BACKUP_DIR.glob(\"*.pdf\")):\n",
    "        print(\"âš ï¸ Backup giÃ  esistente!\")\n",
    "        print(f\"   PDF in backup: {len(list(BACKUP_DIR.glob('*.pdf')))}\")\n",
    "        print(f\"   PDF in inbox: {len(list(INBOX_DIR.glob('*.pdf')))}\")\n",
    "        print(\"\\nğŸ’¡ Usa MODE = 'restore' per ripristinare prima di un nuovo test\")\n",
    "    else:\n",
    "        # Crea backup\n",
    "        BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Trova tutti i PDF e ordinali per dimensione\n",
    "        inbox_pdfs = list(INBOX_DIR.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not inbox_pdfs:\n",
    "            print(\"âš ï¸ Nessun PDF in ptof_inbox/\")\n",
    "        else:\n",
    "            # Ordina per dimensione (piÃ¹ piccoli prima)\n",
    "            inbox_pdfs_sorted = sorted(inbox_pdfs, key=lambda p: p.stat().st_size)\n",
    "            \n",
    "            print(f\"ğŸ“¦ Trovati {len(inbox_pdfs)} PDF\")\n",
    "            print(f\"ğŸ“¦ Seleziono i {NUM_TEST_FILES} piÃ¹ piccoli per il test\")\n",
    "            \n",
    "            # Sposta TUTTI in backup\n",
    "            print(f\"\\nğŸ”„ Sposto tutti i PDF in backup...\")\n",
    "            for pdf in inbox_pdfs:\n",
    "                shutil.move(str(pdf), str(BACKUP_DIR / pdf.name))\n",
    "            print(f\"   âœ… {len(inbox_pdfs)} PDF spostati in ptof_inbox_backup/\")\n",
    "            \n",
    "            # Copia solo i piÃ¹ piccoli in inbox\n",
    "            test_files = inbox_pdfs_sorted[:NUM_TEST_FILES]\n",
    "            print(f\"\\nğŸ“‹ Copio {len(test_files)} PDF per il test:\")\n",
    "            \n",
    "            for i, pdf in enumerate(test_files, 1):\n",
    "                src = BACKUP_DIR / pdf.name\n",
    "                dst = INBOX_DIR / pdf.name\n",
    "                shutil.copy(str(src), str(dst))\n",
    "                size_kb = src.stat().st_size / 1024\n",
    "                print(f\"   {i}. {pdf.name} ({size_kb:.0f} KB)\")\n",
    "            \n",
    "            print(f\"\\nâœ… Setup completato!\")\n",
    "            print(f\"   ğŸ“¥ PDF in inbox (test): {len(list(INBOX_DIR.glob('*.pdf')))}\")\n",
    "            print(f\"   ğŸ“¦ PDF in backup: {len(list(BACKUP_DIR.glob('*.pdf')))}\")\n",
    "            print(f\"\\nğŸš€ Ora esegui la cella 'Workflow Solo Analisi' sotto\")\n",
    "\n",
    "elif MODE == \"restore\":\n",
    "    # =====================================================\n",
    "    # RESTORE: Ripristina tutti i PDF\n",
    "    # =====================================================\n",
    "    \n",
    "    if not BACKUP_DIR.exists():\n",
    "        print(\"âš ï¸ Nessun backup trovato!\")\n",
    "        print(\"ğŸ’¡ Non c'Ã¨ nulla da ripristinare\")\n",
    "    else:\n",
    "        backup_pdfs = list(BACKUP_DIR.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not backup_pdfs:\n",
    "            print(\"âš ï¸ Backup vuoto!\")\n",
    "            # Rimuovi cartella vuota\n",
    "            BACKUP_DIR.rmdir()\n",
    "            print(\"ğŸ—‘ï¸ Cartella backup rimossa\")\n",
    "        else:\n",
    "            print(f\"ğŸ”„ Ripristino {len(backup_pdfs)} PDF dal backup...\")\n",
    "            \n",
    "            # Rimuovi PDF attuali in inbox (sono copie)\n",
    "            for pdf in INBOX_DIR.glob(\"*.pdf\"):\n",
    "                pdf.unlink()\n",
    "            \n",
    "            # Sposta tutti dal backup all'inbox\n",
    "            for pdf in backup_pdfs:\n",
    "                shutil.move(str(pdf), str(INBOX_DIR / pdf.name))\n",
    "            \n",
    "            # Rimuovi cartella backup\n",
    "            BACKUP_DIR.rmdir()\n",
    "            \n",
    "            inbox_count = len(list(INBOX_DIR.glob(\"*.pdf\")))\n",
    "            print(f\"   âœ… {inbox_count} PDF ripristinati in ptof_inbox/\")\n",
    "            print(f\"   ğŸ—‘ï¸ Cartella backup rimossa\")\n",
    "            print(f\"\\nâœ… Restore completato!\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ MODE non valido: {MODE}\")\n",
    "    print(\"ğŸ’¡ Usa 'setup' o 'restore'\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸš€ Workflow Solo Analisi (Senza Validazione)\n",
    "\n",
    "Questa cella esegue il workflow **saltando la pre-validazione**. Usa questa cella se:\n",
    "- I PDF sono giÃ  stati validati con la cella \"Pre-Validazione\"\n",
    "- Sei sicuro che tutti i PDF in inbox sono PTOF validi\n",
    "- Vuoi velocizzare il processo saltando il controllo\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "Nessuna configurazione necessaria. La cella processa automaticamente tutti i PDF in `ptof_inbox/`.\n",
    "\n",
    "### ğŸ“‹ Pipeline\n",
    "\n",
    "| Step | Descrizione |\n",
    "|------|-------------|\n",
    "| 1 | **Conversione PDF â†’ Markdown** (tutti i PDF, nessun filtro) |\n",
    "| 2 | **Analisi Multi-Agente** (Analyst â†’ Reviewer â†’ Refiner) |\n",
    "| 3 | **Archiviazione PDF** in `ptof_processed/` |\n",
    "| 4 | **Rebuild CSV** per la dashboard |\n",
    "\n",
    "### âš ï¸ Differenze dalla cella \"Workflow Completo\"\n",
    "\n",
    "| Aspetto | Workflow Completo | Questa Cella |\n",
    "|---------|-------------------|--------------|\n",
    "| Pre-validazione | âœ… SÃ¬ (heuristics + LLM) | âŒ No |\n",
    "| VelocitÃ  | ğŸ¢ PiÃ¹ lento | âš¡ PiÃ¹ veloce |\n",
    "| Rischio | Basso (scarta non-PTOF) | Medio (processa tutto) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1880549934.py, line 157)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 157\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"   1. Ricarica la dashboard (F5 o Ctrl+R)\")print(\"=\"*70)\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 13: DIAGNOSTICA CSV â†” JSON â†” MIUR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” DIAGNOSTICA COMPLETA: CSV â†” JSON â†” Dashboard â†” Database MIUR\n",
    "# Esegui questa cella per capire discrepanze tra le pagine della Dashboard\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import subprocess\n",
    "\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "os.chdir(BASE_DIR)\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "ANALYSIS_DIR = BASE_DIR / \"analysis_results\"\n",
    "CSV_FILE = BASE_DIR / \"data\" / \"analysis_summary.csv\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” DIAGNOSTICA COMPLETA: CSV â†” JSON â†” Database MIUR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Carica SchoolDatabase\n",
    "try:\n",
    "    from src.utils.school_database import SchoolDatabase\n",
    "    SCHOOL_DB = SchoolDatabase()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ SchoolDatabase non disponibile: {e}\")\n",
    "    SCHOOL_DB = None\n",
    "\n",
    "# =====================================================\n",
    "# 1. ANALISI FILE JSON vs DATABASE MIUR\n",
    "# =====================================================\n",
    "print(\"\\nğŸ“„ 1. CONFRONTO JSON vs DATABASE MIUR\")\n",
    "json_files = list(ANALYSIS_DIR.glob(\"*_analysis.json\"))\n",
    "json_issues = []\n",
    "\n",
    "for jf in json_files:\n",
    "    code = jf.stem.replace(\"_analysis\", \"\").upper()\n",
    "    try:\n",
    "        with open(jf, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        json_meta = data.get('metadata', {})\n",
    "        \n",
    "        # Confronta con database MIUR\n",
    "        if SCHOOL_DB:\n",
    "            db_data = SCHOOL_DB.get_school_data(code)\n",
    "            if db_data:\n",
    "                # Verifica discrepanze nei campi chiave\n",
    "                issues = []\n",
    "                for field in ['comune', 'regione', 'provincia']:\n",
    "                    json_val = str(json_meta.get(field, '')).upper()\n",
    "                    db_val = str(db_data.get(field, '')).upper()\n",
    "                    if json_val and db_val and json_val != db_val and json_val != 'ND':\n",
    "                        issues.append(f\"{field}: JSON='{json_meta.get(field)}' vs MIUR='{db_data.get(field)}'\")\n",
    "                \n",
    "                if issues:\n",
    "                    json_issues.append({\n",
    "                        'code': code,\n",
    "                        'issues': issues,\n",
    "                        'db_data': db_data\n",
    "                    })\n",
    "                    print(f\"   âš ï¸ {code}: {', '.join(issues)}\")\n",
    "                else:\n",
    "                    print(f\"   âœ… {code}: dati coerenti con MIUR\")\n",
    "            else:\n",
    "                print(f\"   â“ {code}: non trovato nel database MIUR\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {code}: errore lettura JSON - {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. ANALISI CSV\n",
    "# =====================================================\n",
    "print(\"\\nğŸ“Š 2. CSV analysis_summary.csv\")\n",
    "csv_data = {}\n",
    "if CSV_FILE.exists():\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    print(f\"   Totale: {len(df)} righe, {len(df.columns)} colonne\")\n",
    "    \n",
    "    # Mostra anteprima\n",
    "    cols = ['school_id', 'denominazione', 'regione', 'comune']\n",
    "    existing = [c for c in cols if c in df.columns]\n",
    "    print(f\"\\n   Anteprima:\")\n",
    "    for _, row in df.head(5).iterrows():\n",
    "        print(f\"     {row.get('school_id', 'N/A')}: {row.get('denominazione', 'N/A')[:35]} ({row.get('regione', 'N/A')})\")\n",
    "else:\n",
    "    print(\"   âŒ CSV non trovato!\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. CORREZIONE AUTOMATICA JSON (opzionale)\n",
    "# =====================================================\n",
    "if json_issues:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ”§ CORREZIONE AUTOMATICA DISPONIBILE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Trovate {len(json_issues)} scuole con dati diversi dal database MIUR\")\n",
    "    print(\"   I JSON verranno corretti con i dati ufficiali del MIUR\")\n",
    "    \n",
    "    FIX_JSON = True  # Imposta False per simulazione\n",
    "    \n",
    "    if FIX_JSON:\n",
    "        fixed = 0\n",
    "        for item in json_issues:\n",
    "            code = item['code']\n",
    "            db_data = item['db_data']\n",
    "            json_path = ANALYSIS_DIR / f\"{code}_analysis.json\"\n",
    "            \n",
    "            try:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Aggiorna metadati con dati MIUR\n",
    "                if 'metadata' not in data:\n",
    "                    data['metadata'] = {}\n",
    "                \n",
    "                data['metadata']['comune'] = db_data.get('comune', data['metadata'].get('comune', 'ND'))\n",
    "                data['metadata']['regione'] = db_data.get('regione', data['metadata'].get('regione', 'ND'))\n",
    "                data['metadata']['provincia'] = db_data.get('provincia', data['metadata'].get('provincia', 'ND'))\n",
    "                data['metadata']['indirizzo'] = db_data.get('indirizzo', data['metadata'].get('indirizzo', 'ND'))\n",
    "                data['metadata']['cap'] = db_data.get('cap', data['metadata'].get('cap', 'ND'))\n",
    "                data['metadata']['email'] = db_data.get('email', data['metadata'].get('email', 'ND'))\n",
    "                data['metadata']['pec'] = db_data.get('pec', data['metadata'].get('pec', 'ND'))\n",
    "                data['metadata']['statale_paritaria'] = db_data.get('statale_paritaria', data['metadata'].get('statale_paritaria', 'ND'))\n",
    "                \n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                print(f\"   âœ… {code}: JSON aggiornato con dati MIUR\")\n",
    "                fixed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ {code}: errore - {e}\")\n",
    "        \n",
    "        if fixed > 0:\n",
    "            print(f\"\\n   ğŸ“Š Rigenero CSV per riflettere le correzioni...\")\n",
    "            result = subprocess.run(['python', 'src/processing/rebuild_csv.py'], \n",
    "                                   capture_output=True, text=True, cwd=str(BASE_DIR))\n",
    "            if result.returncode == 0:\n",
    "                print(\"   âœ… CSV rigenerato!\")\n",
    "            else:\n",
    "                print(f\"   âŒ Errore: {result.stderr}\")\n",
    "else:\n",
    "    print(\"\\nâœ… Tutti i JSON sono coerenti con il database MIUR!\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. RIEPILOGO FINALE\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ RIEPILOGO\")\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… FIX APPLICATA: La pagina 'Dati Grezzi' ora usa la regione dal CSV\")\n",
    "print(\"   (non sovrascrive piÃ¹ con region_map.json)\")\n",
    "print(\"\")\n",
    "print(\"Per vedere i dati aggiornati nella dashboard:\")\n",
    "print(\"   1. Ricarica la dashboard (F5 o Ctrl+R)\")\n",
    "print(\"=\"*70)\n",
    "print(\"   2. La cache TTL Ã¨ di 60 secondi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ WORKFLOW COMPLETO ANALISI PTOF\n",
      "ğŸ• 2025-12-22 17:06:50\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ Caricamento database MIUR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 17:06:50,604 - INFO - [SchoolDatabase] Loaded 61855 schools.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchoolDatabase] Loaded 61855 schools from CSVs.\n",
      "   âœ… Caricato: 61855 scuole\n",
      "   âœ… Metodo get_location_by_comune: presente\n",
      "\n",
      "ğŸ“¥ PDF in inbox: 0\n",
      "âš ï¸ Nessun PDF da processare!\n",
      "ğŸ’¡ Copia i PDF in ptof_inbox/ e riprova\n",
      "\n",
      "======================================================================\n",
      "ğŸ”§ STEP 4: Verifica e Fix Metadati (tutti i JSON)\n",
      "======================================================================\n",
      "   ğŸ”§ BNIS01100L_analysis: 4 fix\n",
      "      â€¢ school_id = BNIS01100L\n",
      "      â€¢ provincia = 'Lecce' (da comune)\n",
      "      â€¢ regione = 'Puglia' (da comune)\n",
      "      â€¢ nome_scuola = 'Liceo â€œVirgilio-Redi\"...'\n",
      "   ğŸ”§ NAIC80800G_analysis: 3 fix\n",
      "      â€¢ provincia = 'Perugia' (da comune)\n",
      "      â€¢ regione = 'Umbria' (da comune)\n",
      "      â€¢ nome_scuola = 'Istituto Istruzione Superiore Marco Polo...'\n",
      "   ğŸ”§ PCPC00500A_analysis: 3 fix\n",
      "      â€¢ provincia = 'Piacenza' (da comune)\n",
      "      â€¢ regione = 'Emilia Romagna' (da comune)\n",
      "      â€¢ nome_scuola = 'Liceo San Benedetto...'\n",
      "   ğŸ”§ BS1M004009_analysis: 3 fix\n",
      "      â€¢ provincia = 'Brescia' (da comune)\n",
      "      â€¢ regione = 'Lombardia' (da comune)\n",
      "      â€¢ nome_scuola = 'Liceo Paritario â€œMadonna della Neveâ€...'\n",
      "\n",
      "   âœ… Corretti 4 file\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š STEP 5: Rebuild CSV Dashboard\n",
      "======================================================================\n",
      "   âœ… CSV aggiornato: 10 scuole\n",
      "\n",
      "   Campi principali:\n",
      "      â€¢ nome_scuola: 0/10 compilati\n",
      "      â€¢ comune: 10/10 compilati\n",
      "      â€¢ provincia: 10/10 compilati\n",
      "      â€¢ regione: 10/10 compilati\n",
      "      â€¢ area_geografica: 10/10 compilati\n",
      "      â€¢ territorio: 10/10 compilati\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š RIEPILOGO FINALE\n",
      "======================================================================\n",
      "   ğŸ“¥ PDF in inbox: 0\n",
      "   âœ… PDF processati: 10\n",
      "   ğŸ“ File Markdown: 10\n",
      "   ğŸ“Š Analisi JSON: 10\n",
      "\n",
      "ğŸ“‹ Stato metadati per ogni scuola:\n",
      "   âœ… BNIS01100L: Lecce, Lecce (Sud/Non Metropolitano)\n",
      "   âœ… BS1M004009: Adro, Brescia (Nord/Non Metropolitano)\n",
      "   âœ… MIPC09500C: Milano, Milano (Nord/Metropolitano)\n",
      "   âœ… NAIC80800G: Assisi, Perugia (Centro/Non Metropolitano)\n",
      "   âœ… PCPC00500A: Piacenza, Piacenza (Nord/Non Metropolitano)\n",
      "   âœ… RMIC8C600P: Cave, Roma (Centro/Non Metropolitano)\n",
      "   âœ… RMPL355003: ROMA, Roma (Centro/Metropolitano)\n",
      "   âœ… UD1M00600L: Udine, Udine (Nord/Non Metropolitano)\n",
      "   âœ… VAPLVL5003: Merate, Varese (Nord/Non Metropolitano)\n",
      "   âœ… VR1A23500E: Verona, Verona (Nord/Metropolitano)\n",
      "\n",
      "======================================================================\n",
      "âœ… WORKFLOW COMPLETATO\n",
      "======================================================================\n",
      "ğŸ”„ Ricarica la dashboard per vedere i nuovi dati\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 14: WORKFLOW COMPLETO ANALISI PTOF\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ PDF â†’ MD â†’ Analisi Multi-Agente â†’ Arricchimento MIUR â†’ CSV Dashboard\n",
    "# âœ… Include: validazione codici, arricchimento MIUR, calcolo area_geo/territorio\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurazione\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "os.chdir(BASE_DIR)\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "INBOX_DIR = BASE_DIR / \"ptof_inbox\"\n",
    "PROCESSED_DIR = BASE_DIR / \"ptof_processed\"\n",
    "MD_DIR = BASE_DIR / \"ptof_md\"\n",
    "ANALYSIS_DIR = BASE_DIR / \"analysis_results\"\n",
    "CSV_FILE = BASE_DIR / \"data\" / \"analysis_summary.csv\"\n",
    "\n",
    "# Crea directory\n",
    "for d in [INBOX_DIR, PROCESSED_DIR, MD_DIR, ANALYSIS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ WORKFLOW COMPLETO ANALISI PTOF\")\n",
    "print(f\"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =====================================================\n",
    "# COSTANTI PER AREA_GEOGRAFICA E TERRITORIO\n",
    "# =====================================================\n",
    "# Funzione per normalizzare regione e ottenere area geografica\n",
    "def get_area_geografica(regione: str) -> str:\n",
    "    \"\"\"Ritorna Nord/Centro/Sud/Isole dalla regione (case-insensitive)\"\"\"\n",
    "    if not regione or regione in ['ND', '', None]:\n",
    "        return 'ND'\n",
    "    \n",
    "    r = regione.upper().strip()\n",
    "    \n",
    "    # Nord\n",
    "    if any(x in r for x in ['PIEMONTE', 'LOMBARDIA', 'VENETO', 'LIGURIA', \n",
    "                            'EMILIA', 'FRIULI', 'TRENTINO', 'VALLE']):\n",
    "        return 'Nord'\n",
    "    # Centro\n",
    "    if any(x in r for x in ['TOSCANA', 'UMBRIA', 'MARCHE', 'LAZIO']):\n",
    "        return 'Centro'\n",
    "    # Sud\n",
    "    if any(x in r for x in ['ABRUZZO', 'MOLISE', 'CAMPANIA', 'PUGLIA', \n",
    "                            'BASILICATA', 'CALABRIA']):\n",
    "        return 'Sud'\n",
    "    # Isole\n",
    "    if any(x in r for x in ['SICILIA', 'SARDEGNA']):\n",
    "        return 'Isole'\n",
    "    \n",
    "    return 'ND'\n",
    "\n",
    "PROVINCE_METRO = ['Roma', 'Milano', 'Napoli', 'Torino', 'Bari', 'Firenze', \n",
    "                  'Bologna', 'Genova', 'Venezia', 'Palermo', 'Catania', \n",
    "                  'Messina', 'Reggio Calabria', 'Cagliari']\n",
    "\n",
    "# =====================================================\n",
    "# FUNZIONE: Arricchimento e fix metadati completo\n",
    "# =====================================================\n",
    "def complete_metadata_fix(json_path: Path, school_db=None) -> dict:\n",
    "    \"\"\"\n",
    "    Arricchimento completo metadati:\n",
    "    1. Carica dati MIUR se disponibili\n",
    "    2. Fallback: deriva provincia/regione dal comune\n",
    "    3. Calcola area_geografica da regione\n",
    "    4. Calcola territorio da provincia\n",
    "    5. Fix nome_scuola da denominazione\n",
    "    \"\"\"\n",
    "    result = {'changes': [], 'warnings': []}\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'metadata' not in data:\n",
    "        data['metadata'] = {}\n",
    "    meta = data['metadata']\n",
    "    changed = False\n",
    "    \n",
    "    # Estrai codice dal nome file\n",
    "    school_code = json_path.stem.replace('_analysis', '')\n",
    "    if meta.get('school_id') != school_code:\n",
    "        meta['school_id'] = school_code\n",
    "        meta['codice_meccanografico'] = school_code\n",
    "        result['changes'].append(f\"school_id = {school_code}\")\n",
    "        changed = True\n",
    "    \n",
    "    # FASE 1: Cerca dati MIUR\n",
    "    if school_db:\n",
    "        miur_data = school_db.get_school_data(school_code)\n",
    "        if miur_data:\n",
    "            miur_fields = {\n",
    "                'nome_scuola': miur_data.get('denominazione', ''),\n",
    "                'denominazione': miur_data.get('denominazione', ''),\n",
    "                'comune': miur_data.get('comune', ''),\n",
    "                'provincia': miur_data.get('provincia', ''),\n",
    "                'regione': miur_data.get('regione', ''),\n",
    "                'tipologia': miur_data.get('tipo_scuola', ''),\n",
    "            }\n",
    "            for key, val in miur_fields.items():\n",
    "                if val and val not in ['ND', '', None]:\n",
    "                    if meta.get(key) in ['ND', '', None]:\n",
    "                        meta[key] = val\n",
    "                        result['changes'].append(f\"{key} = '{val}' (MIUR)\")\n",
    "                        changed = True\n",
    "        else:\n",
    "            result['warnings'].append(f\"Codice {school_code} non in database MIUR\")\n",
    "            \n",
    "            # FALLBACK: Deriva provincia/regione dal comune\n",
    "            comune = meta.get('comune', '')\n",
    "            if comune and comune not in ['ND', '', None] and school_db:\n",
    "                location = school_db.get_location_by_comune(comune)\n",
    "                if location:\n",
    "                    if meta.get('provincia') in ['ND', '', None]:\n",
    "                        meta['provincia'] = location['provincia']\n",
    "                        result['changes'].append(f\"provincia = '{location['provincia']}' (da comune)\")\n",
    "                        changed = True\n",
    "                    if meta.get('regione') in ['ND', '', None]:\n",
    "                        meta['regione'] = location['regione']\n",
    "                        result['changes'].append(f\"regione = '{location['regione']}' (da comune)\")\n",
    "                        changed = True\n",
    "    \n",
    "    # FASE 2: Fix nome_scuola da denominazione\n",
    "    if meta.get('nome_scuola') in ['ND', '', None]:\n",
    "        denom = meta.get('denominazione', '')\n",
    "        if denom and denom not in ['ND', '', None]:\n",
    "            meta['nome_scuola'] = denom\n",
    "            result['changes'].append(f\"nome_scuola = '{denom[:40]}...'\")\n",
    "            changed = True\n",
    "    \n",
    "    # FASE 3: Calcola area_geografica da regione (case-insensitive)\n",
    "    regione = meta.get('regione', '')\n",
    "    if regione and regione not in ['ND', '', None]:\n",
    "        area_geo = get_area_geografica(regione)\n",
    "        if area_geo != 'ND':\n",
    "            current_area = meta.get('area_geografica', '')\n",
    "            # Correggi se mancante o errato\n",
    "            if current_area in ['ND', '', None, 'Metropolitano', 'Non Metropolitano'] or current_area != area_geo:\n",
    "                meta['area_geografica'] = area_geo\n",
    "                result['changes'].append(f\"area_geografica = '{area_geo}' (da {regione})\")\n",
    "                changed = True\n",
    "    \n",
    "    # FASE 4: Calcola territorio da provincia (case-insensitive)\n",
    "    provincia = meta.get('provincia', '')\n",
    "    if provincia and provincia not in ['ND', '', None]:\n",
    "        # Confronto case-insensitive per le province metropolitane\n",
    "        prov_upper = provincia.upper().strip()\n",
    "        is_metro = any(p.upper() == prov_upper for p in PROVINCE_METRO)\n",
    "        territorio = 'Metropolitano' if is_metro else 'Non Metropolitano'\n",
    "        current_terr = meta.get('territorio', '')\n",
    "        # Correggi se mancante o errato\n",
    "        if current_terr in ['ND', '', None, 'Nord', 'Centro', 'Sud', 'Isole'] or current_terr != territorio:\n",
    "            meta['territorio'] = territorio\n",
    "            result['changes'].append(f\"territorio = '{territorio}' (da {provincia})\")\n",
    "            changed = True\n",
    "    \n",
    "    # Salva se modificato\n",
    "    if changed:\n",
    "        data['metadata'] = meta\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# INIZIALIZZAZIONE DATABASE MIUR (con reload forzato)\n",
    "# =====================================================\n",
    "print(\"\\nğŸ”§ Caricamento database MIUR...\")\n",
    "import importlib\n",
    "import src.utils.school_database as school_db_module\n",
    "importlib.reload(school_db_module)\n",
    "from src.utils.school_database import SchoolDatabase\n",
    "\n",
    "# Reset singleton per forzare ricaricamento\n",
    "SchoolDatabase._instance = None\n",
    "SchoolDatabase._loaded = False\n",
    "SCHOOL_DB = SchoolDatabase()\n",
    "print(f\"   âœ… Caricato: {len(SCHOOL_DB._data)} scuole\")\n",
    "print(f\"   âœ… Metodo get_location_by_comune: {'presente' if hasattr(SCHOOL_DB, 'get_location_by_comune') else 'MANCANTE!'}\")\n",
    "\n",
    "# Conta PDF\n",
    "inbox_pdfs = list(INBOX_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nğŸ“¥ PDF in inbox: {len(inbox_pdfs)}\")\n",
    "\n",
    "if not inbox_pdfs:\n",
    "    print(\"âš ï¸ Nessun PDF da processare!\")\n",
    "    print(\"ğŸ’¡ Copia i PDF in ptof_inbox/ e riprova\")\n",
    "else:\n",
    "    # =====================================================\n",
    "    # STEP 0: VALIDAZIONE PRE-ANALISI\n",
    "    # =====================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ” STEP 0: Validazione codici meccanografici\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    valid_pdfs = []\n",
    "    validation_warnings = []\n",
    "    \n",
    "    # Pattern codice: 2 lettere + 2 alfanum + 6 alfanum (es: MIPC09500C)\n",
    "    code_pattern = re.compile(r'([A-Z]{2}[A-Z0-9]{2}[A-Z0-9]{6})', re.IGNORECASE)\n",
    "    \n",
    "    for pdf_path in inbox_pdfs:\n",
    "        match = code_pattern.search(pdf_path.stem.upper())\n",
    "        if match:\n",
    "            school_code = match.group(1).upper()\n",
    "            miur_data = SCHOOL_DB.get_school_data(school_code)\n",
    "            \n",
    "            if miur_data:\n",
    "                print(f\"âœ… {school_code}: {miur_data.get('denominazione', 'ND')[:50]}\")\n",
    "                valid_pdfs.append((pdf_path, school_code, miur_data))\n",
    "            else:\n",
    "                print(f\"âš ï¸ {school_code}: Non in MIUR (procedo comunque)\")\n",
    "                valid_pdfs.append((pdf_path, school_code, None))\n",
    "                validation_warnings.append(school_code)\n",
    "        else:\n",
    "            print(f\"âŒ {pdf_path.name}: Codice non estratto\")\n",
    "    \n",
    "    if validation_warnings:\n",
    "        print(f\"\\nâš ï¸ {len(validation_warnings)} codici non trovati in database MIUR\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ PDF da processare: {len(valid_pdfs)}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # STEP 1: CONVERSIONE PDF â†’ MARKDOWN\n",
    "    # =====================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“ STEP 1: Conversione PDF â†’ Markdown\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from src.processing.convert_pdfs_to_md import pdf_to_markdown\n",
    "    \n",
    "    converted = []\n",
    "    \n",
    "    for pdf_path, school_code, miur_data in valid_pdfs:\n",
    "        md_output = MD_DIR / f\"{school_code}.md\"\n",
    "        \n",
    "        # Verifica se giÃ  analizzato\n",
    "        analysis_file = ANALYSIS_DIR / f\"{school_code}_analysis.json\"\n",
    "        if analysis_file.exists():\n",
    "            print(f\"â­ï¸ GiÃ  analizzato: {school_code}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"ğŸ”„ Convertendo: {pdf_path.name} â†’ {school_code}.md\")\n",
    "        \n",
    "        try:\n",
    "            if pdf_to_markdown(str(pdf_path), str(md_output)):\n",
    "                converted.append((pdf_path, school_code, miur_data))\n",
    "                print(f\"   âœ… Convertito!\")\n",
    "            else:\n",
    "                print(f\"   âŒ Errore conversione\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Errore: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Convertiti: {len(converted)} file\")\n",
    "    \n",
    "    if converted:\n",
    "        # =====================================================\n",
    "        # STEP 2: ANALISI MULTI-AGENTE\n",
    "        # =====================================================\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ¤– STEP 2: Analisi Multi-Agente\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        from app.agentic_pipeline import (\n",
    "            AnalystAgent, ReviewerAgent, RefinerAgent, SynthesizerAgent,\n",
    "            process_single_ptof\n",
    "        )\n",
    "        \n",
    "        # Inizializza agenti\n",
    "        print(\"ğŸ”§ Inizializzazione agenti...\")\n",
    "        analyst = AnalystAgent()\n",
    "        reviewer = ReviewerAgent()\n",
    "        refiner = RefinerAgent()\n",
    "        synthesizer = SynthesizerAgent()\n",
    "        print(\"   âœ… Agenti pronti!\")\n",
    "        \n",
    "        analyzed = []\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        for pdf_path, school_code, miur_data in converted:\n",
    "            md_file = MD_DIR / f\"{school_code}.md\"\n",
    "            \n",
    "            if not md_file.exists():\n",
    "                print(f\"âš ï¸ MD non trovato: {school_code}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸ”„ Analizzando: {school_code}\")\n",
    "            \n",
    "            # Info MIUR\n",
    "            if miur_data:\n",
    "                print(f\"   ğŸ“ MIUR: {miur_data.get('denominazione', 'ND')[:40]} - {miur_data.get('comune', 'ND')}\")\n",
    "            \n",
    "            def status_callback(msg):\n",
    "                if any(x in msg for x in ['Analyst', 'Reviewer', 'Refiner', 'chunk']):\n",
    "                    print(f\"   ğŸ¤– {msg}\")\n",
    "            \n",
    "            try:\n",
    "                result = process_single_ptof(\n",
    "                    md_file=str(md_file),\n",
    "                    analyst=analyst,\n",
    "                    reviewer=reviewer,\n",
    "                    refiner=refiner,\n",
    "                    synthesizer=synthesizer,\n",
    "                    status_callback=status_callback\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    analyzed.append(school_code)\n",
    "                    print(f\"   âœ… Analisi completata\")\n",
    "                    \n",
    "                    # ARRICCHIMENTO MIUR + FIX METADATI\n",
    "                    json_path = ANALYSIS_DIR / f\"{school_code}_analysis.json\"\n",
    "                    if json_path.exists():\n",
    "                        fix_result = complete_metadata_fix(json_path, SCHOOL_DB)\n",
    "                        if fix_result['changes']:\n",
    "                            print(f\"   ğŸ“ Arricchito: {len(fix_result['changes'])} campi\")\n",
    "                            for c in fix_result['changes'][:3]:\n",
    "                                print(f\"      â€¢ {c}\")\n",
    "                        for w in fix_result['warnings']:\n",
    "                            print(f\"   âš ï¸ {w}\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ Nessun risultato\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Errore: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\nğŸ“Š Analizzati: {len(analyzed)} file in {elapsed:.1f}s\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # STEP 3: ARCHIVIAZIONE\n",
    "        # =====================================================\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“¦ STEP 3: Archiviazione PDF\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        batch_dir = PROCESSED_DIR / f\"batch_{timestamp}\"\n",
    "        batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for pdf_path, school_code, miur_data in converted:\n",
    "            try:\n",
    "                dest = batch_dir / pdf_path.name\n",
    "                shutil.move(str(pdf_path), str(dest))\n",
    "                print(f\"   âœ… {pdf_path.name} â†’ processed/\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Errore: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 4: FIX METADATI SU TUTTI I JSON\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”§ STEP 4: Verifica e Fix Metadati (tutti i JSON)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "json_files = list(ANALYSIS_DIR.glob(\"*.json\"))\n",
    "fixed_count = 0\n",
    "for jf in json_files:\n",
    "    fix_result = complete_metadata_fix(jf, SCHOOL_DB)\n",
    "    if fix_result['changes']:\n",
    "        print(f\"   ğŸ”§ {jf.stem}: {len(fix_result['changes'])} fix\")\n",
    "        for c in fix_result['changes']:\n",
    "            print(f\"      â€¢ {c}\")\n",
    "        fixed_count += 1\n",
    "\n",
    "if fixed_count:\n",
    "    print(f\"\\n   âœ… Corretti {fixed_count} file\")\n",
    "else:\n",
    "    print(\"   âœ… Tutti i metadati sono giÃ  completi\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 5: REBUILD CSV DASHBOARD\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š STEP 5: Rebuild CSV Dashboard\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['python3', 'src/processing/rebuild_csv.py'],\n",
    "    capture_output=True, text=True, timeout=120, cwd=str(BASE_DIR)\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    import pandas as pd\n",
    "    if CSV_FILE.exists():\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "        print(f\"   âœ… CSV aggiornato: {len(df)} scuole\")\n",
    "        print(f\"\\n   Campi principali:\")\n",
    "        for col in ['nome_scuola', 'comune', 'provincia', 'regione', 'area_geografica', 'territorio']:\n",
    "            non_empty = df[col].notna().sum() if col in df.columns else 0\n",
    "            print(f\"      â€¢ {col}: {non_empty}/{len(df)} compilati\")\n",
    "else:\n",
    "    print(f\"   âŒ Errore: {result.stderr}\")\n",
    "\n",
    "# =====================================================\n",
    "# RIEPILOGO FINALE\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š RIEPILOGO FINALE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "inbox_count = len(list(INBOX_DIR.glob(\"*.pdf\")))\n",
    "processed_count = len(list(PROCESSED_DIR.rglob(\"*.pdf\")))\n",
    "md_count = len(list(MD_DIR.glob(\"*.md\")))\n",
    "analysis_count = len(list(ANALYSIS_DIR.glob(\"*.json\")))\n",
    "\n",
    "print(f\"   ğŸ“¥ PDF in inbox: {inbox_count}\")\n",
    "print(f\"   âœ… PDF processati: {processed_count}\")\n",
    "print(f\"   ğŸ“ File Markdown: {md_count}\")\n",
    "print(f\"   ğŸ“Š Analisi JSON: {analysis_count}\")\n",
    "\n",
    "# Verifica metadati completi\n",
    "print(\"\\nğŸ“‹ Stato metadati per ogni scuola:\")\n",
    "REQUIRED_FIELDS = ['nome_scuola', 'comune', 'provincia', 'regione', 'area_geografica', 'territorio']\n",
    "for jf in sorted(ANALYSIS_DIR.glob(\"*.json\")):\n",
    "    with open(jf, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    meta = data.get('metadata', {})\n",
    "    school_id = meta.get('school_id', jf.stem)\n",
    "    missing = [k for k in REQUIRED_FIELDS if meta.get(k) in ['ND', '', None]]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"   âš ï¸ {school_id}: mancano {missing}\")\n",
    "    else:\n",
    "        info = f\"{meta.get('comune')}, {meta.get('provincia')}\"\n",
    "        geo = f\"{meta.get('area_geografica')}/{meta.get('territorio')}\"\n",
    "        print(f\"   âœ… {school_id}: {info} ({geo})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… WORKFLOW COMPLETATO\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”„ Ricarica la dashboard per vedere i nuovi dati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Post-Analisi: Scova Incongruenze nei Nomi\n",
    "\n",
    "Questa cella verifica la coerenza tra i **nomi dei file** e i **codici scuola** nei metadati.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "Nessuna configurazione necessaria.\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. **Scansiona JSON**: Legge tutti i file in `analysis_results/`\n",
    "2. **Confronta nomi**: Verifica che il nome del file corrisponda a `metadata.school_id`\n",
    "3. **Segnala problemi**:\n",
    "   - `rename_json`: Nome file diverso dal codice scuola\n",
    "   - `conflict_json`: File con nome corretto giÃ  esistente\n",
    "   - `invalid_school_id`: Codice scuola mancante o non valido\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Dopo un batch di analisi per verificare consistenza\n",
    "- Se noti problemi con i codici scuola nella dashboard\n",
    "- Prima di archiviare i risultati finali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale incongruenze trovate: 3\n",
      "[01] rename_json | AGPC010001_PTOF_analysis.json -> AGPC010001_analysis.json | nome base AGPC010001_PTOF != metadata.school_id\n",
      "[02] rename_analysis_md | AGPC010001_PTOF_analysis.md -> AGPC010001_analysis.md | nome base AGPC010001_PTOF != metadata.school_id\n",
      "[03] rename_md | AGPC010001_PTOF.md -> AGPC010001.md | nome base AGPC010001_PTOF != metadata.school_id\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 16: POST-ANALISI INCONGRUENZE NOMI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"/Users/danieledragoni/git/LIste\")\n",
    "if not (BASE_DIR / \"analysis_results\").exists():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    if not (BASE_DIR / \"analysis_results\").exists():\n",
    "        BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "ANALYSIS_DIR = str(BASE_DIR / \"analysis_results\")\n",
    "MD_DIR = str(BASE_DIR / \"ptof_md\")\n",
    "\n",
    "\n",
    "\n",
    "def normalize_school_id(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "def is_valid_school_id(value):\n",
    "    value = normalize_school_id(value)\n",
    "    if not value or value.upper() in {\"ND\", \"N/A\", \"NONE\", \"UNKNOWN\"}:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def build_issues():\n",
    "    issues = []\n",
    "    json_files = glob.glob(os.path.join(ANALYSIS_DIR, \"*_analysis.json\"))\n",
    "    for json_path in sorted(json_files):\n",
    "        filename = os.path.basename(json_path)\n",
    "        base = filename.replace(\"_analysis.json\", \"\")\n",
    "        try:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            issues.append({\n",
    "                \"kind\": \"json_read_error\",\n",
    "                \"current\": json_path,\n",
    "                \"target\": \"\",\n",
    "                \"reason\": f\"json read error: {e}\",\n",
    "            })\n",
    "            continue\n",
    "        school_id = data.get(\"metadata\", {}).get(\"school_id\")\n",
    "        if not is_valid_school_id(school_id):\n",
    "            issues.append({\n",
    "                \"kind\": \"invalid_school_id\",\n",
    "                \"current\": json_path,\n",
    "                \"target\": \"\",\n",
    "                \"reason\": f\"metadata.school_id non valido: {school_id}\",\n",
    "            })\n",
    "            continue\n",
    "        school_id = normalize_school_id(school_id)\n",
    "        if base != school_id:\n",
    "            target_json = os.path.join(ANALYSIS_DIR, f\"{school_id}_analysis.json\")\n",
    "            if os.path.exists(target_json):\n",
    "                issues.append({\n",
    "                    \"kind\": \"conflict_json\",\n",
    "                    \"current\": json_path,\n",
    "                    \"target\": target_json,\n",
    "                    \"reason\": f\"dest esiste per {school_id}\",\n",
    "                })\n",
    "            else:\n",
    "                issues.append({\n",
    "                    \"kind\": \"rename_json\",\n",
    "                    \"current\": json_path,\n",
    "                    \"target\": target_json,\n",
    "                    \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                })\n",
    "\n",
    "            old_analysis_md = os.path.join(ANALYSIS_DIR, f\"{base}_analysis.md\")\n",
    "            new_analysis_md = os.path.join(ANALYSIS_DIR, f\"{school_id}_analysis.md\")\n",
    "            if os.path.exists(old_analysis_md):\n",
    "                if os.path.exists(new_analysis_md):\n",
    "                    issues.append({\n",
    "                        \"kind\": \"conflict_analysis_md\",\n",
    "                        \"current\": old_analysis_md,\n",
    "                        \"target\": new_analysis_md,\n",
    "                        \"reason\": f\"dest esiste per {school_id}\",\n",
    "                    })\n",
    "                else:\n",
    "                    issues.append({\n",
    "                        \"kind\": \"rename_analysis_md\",\n",
    "                        \"current\": old_analysis_md,\n",
    "                        \"target\": new_analysis_md,\n",
    "                        \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                    })\n",
    "\n",
    "            old_md = os.path.join(MD_DIR, f\"{base}.md\")\n",
    "            new_md = os.path.join(MD_DIR, f\"{school_id}.md\")\n",
    "            if os.path.exists(old_md):\n",
    "                if os.path.exists(new_md):\n",
    "                    issues.append({\n",
    "                        \"kind\": \"conflict_md\",\n",
    "                        \"current\": old_md,\n",
    "                        \"target\": new_md,\n",
    "                        \"reason\": f\"dest esiste per {school_id}\",\n",
    "                    })\n",
    "                else:\n",
    "                    issues.append({\n",
    "                        \"kind\": \"rename_md\",\n",
    "                        \"current\": old_md,\n",
    "                        \"target\": new_md,\n",
    "                        \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                    })\n",
    "    return issues\n",
    "\n",
    "issues = build_issues()\n",
    "print(f\"Totale incongruenze trovate: {len(issues)}\")\n",
    "for idx, issue in enumerate(issues, 1):\n",
    "\n",
    "    target = issue.get(\"target\") or \"-\"\n",
    "    print(f\"[{idx:02d}] {issue['kind']} | {os.path.basename(issue['current'])} -> {os.path.basename(target)} | {issue['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”§ Correggi Incongruenze nei Nomi\n",
    "\n",
    "Questa cella permette di **rinominare i file** per allinearli ai codici scuola corretti.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Descrizione | Valore Default |\n",
    "|-----------|-------------|----------------|\n",
    "| `dry_run` | Simula senza rinominare effettivamente | `True` |\n",
    "| `selection` | ID da correggere (es. \"1,3,5\", \"all\", \"none\") | Input interattivo |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. **Mostra piano**: Elenca le rinominazioni proposte\n",
    "2. **Chiede conferma**: Input interattivo per selezionare quali file rinominare\n",
    "3. **Esegue rinomina**: Solo se `dry_run = False`\n",
    "\n",
    "### âš ï¸ Note\n",
    "\n",
    "- Imposta `dry_run = False` nel codice per rinominare effettivamente\n",
    "- Non sovrascrive file esistenti (skip automatico)\n",
    "- Dopo la rinomina, rigenera il CSV con la cella \"Diagnostica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piano: 1) selezione ID  2) rinomina  3) riepilogo\n",
      "[DRY] AGPC010001_PTOF_analysis.json -> AGPC010001_analysis.json\n",
      "[DRY] AGPC010001_PTOF_analysis.md -> AGPC010001_analysis.md\n",
      "[DRY] AGPC010001_PTOF.md -> AGPC010001.md\n",
      "Rinominati: 3 (dry_run=True)\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 18: CORREGGI INCONGRUENZE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def safe_rename(src, dst, dry_run=False):\n",
    "    if os.path.abspath(src) == os.path.abspath(dst):\n",
    "        return False\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"âš ï¸ Skip rename (dest exists): {os.path.basename(dst)}\")\n",
    "        return False\n",
    "    if dry_run:\n",
    "        print(f\"[DRY] {os.path.basename(src)} -> {os.path.basename(dst)}\")\n",
    "        return True\n",
    "    os.rename(src, dst)\n",
    "    print(f\"âœ… Renamed: {os.path.basename(src)} -> {os.path.basename(dst)}\")\n",
    "    return True\n",
    "\n",
    "if not issues:\n",
    "    print(\"Nessuna incongruenza da correggere.\")\n",
    "else:\n",
    "    print(\"Piano: 1) selezione ID  2) rinomina  3) riepilogo\")\n",
    "    selection = input(\"Quali ID vuoi correggere? (es. 1,3,5 | all | none): \").strip().lower()\n",
    "    if selection in {\"\", \"none\", \"no\"}:\n",
    "        print(\"Nessuna correzione eseguita.\")\n",
    "    else:\n",
    "        if selection == \"all\":\n",
    "            ids = list(range(1, len(issues) + 1))\n",
    "        else:\n",
    "            ids = []\n",
    "            for part in selection.split(','):\n",
    "                part = part.strip()\n",
    "                if part.isdigit():\n",
    "                    ids.append(int(part))\n",
    "        dry_run = True\n",
    "        renamed = 0\n",
    "        for idx in ids:\n",
    "            if idx < 1 or idx > len(issues):\n",
    "                continue\n",
    "            issue = issues[idx - 1]\n",
    "            if not issue['kind'].startswith('rename_'):\n",
    "                print(f\"- Skip {idx:02d} ({issue['kind']})\")\n",
    "                continue\n",
    "            if safe_rename(issue['current'], issue['target'], dry_run=dry_run):\n",
    "\n",
    "                renamed += 1\n",
    "        print(f\"Rinominati: {renamed} (dry_run={dry_run})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Verifica Stato Directory\n",
    "\n",
    "Mostra un riepilogo veloce del numero di file in ogni cartella del progetto.\n",
    "\n",
    "### ğŸ“‹ Output\n",
    "\n",
    "| Directory | Descrizione |\n",
    "|-----------|-------------|\n",
    "| `ptof_inbox/` | PDF in attesa di processamento |\n",
    "| `ptof_processed/` | PDF giÃ  analizzati |\n",
    "| `ptof_md/` | File Markdown convertiti |\n",
    "| `analysis_results/` | Analisi JSON completate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 20: VERIFICA STATO DIRECTORY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "%%bash\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "\n",
    "echo \"ğŸ“Š STATO DIRECTORY\"\n",
    "echo \"========================================\"\n",
    "echo \"ğŸ“¥ Inbox PDF: $(find ptof_inbox -name '*.pdf' 2>/dev/null | wc -l)\"\n",
    "echo \"âœ… Processed PDF: $(find ptof_processed -name '*.pdf' 2>/dev/null | wc -l)\"\n",
    "\n",
    "echo \"ğŸ“ Markdown: $(find ptof_md -name '*.md' 2>/dev/null | wc -l)\"echo \"========================================\"\n",
    "echo \"ğŸ“Š Analisi JSON: $(find analysis_results -name '*.json' 2>/dev/null | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”— Analisi Multi-Agente Batch (Bash)\n",
    "\n",
    "Esegue la pipeline multi-agente usando comandi bash. Utile per:\n",
    "- Debug e testing\n",
    "- Esecuzione da terminale\n",
    "- Log piÃ¹ dettagliati\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "Modifica il path nel comando se necessario.\n",
    "\n",
    "### ğŸ“‹ Pipeline\n",
    "\n",
    "```\n",
    "Analyst â†’ Reviewer â†’ Refiner â†’ Synthesizer\n",
    "```\n",
    "\n",
    "### âš ï¸ Nota\n",
    "\n",
    "Preferisci le celle Python sopra per un controllo migliore. Questa cella Ã¨ per uso avanzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 22: PIPELINE MULTI-AGENTE BATCH (BASH)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "%%bash\n",
    "# Pipeline Multi-Agente - Batch completo su cartella ptof_md/\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"ğŸš€ Avvio Pipeline Multi-Agente Batch\"\n",
    "echo \"ğŸ“‚ Directory: ptof_md/\"\n",
    "echo \"ğŸ¤– Agenti: Analyst â†’ Reviewer â†’ Refiner\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python app/agentic_pipeline.py 2>&1 | tee logs/agentic_pipeline.log\n",
    "\n",
    "\n",
    "echo \"========================================\"echo \"ğŸ“‹ Log: logs/agentic_pipeline.log\"\n",
    "echo \"âœ… Pipeline completata!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Analisi Singolo File (con Logging)\n",
    "\n",
    "Processa un **singolo file PTOF** con logging dettagliato. Utile per debug e test.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Posizione | Descrizione |\n",
    "|-----------|-----------|-------------|\n",
    "| `md_file` | Nel codice | Path al file markdown da analizzare |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. Inizializza i 4 agenti (Analyst, Reviewer, Refiner, Synthesizer)\n",
    "2. Processa il singolo file\n",
    "3. Salva log in `logs/single_analysis.log`\n",
    "\n",
    "### âš ï¸ Nota\n",
    "\n",
    "Questa cella **non aggiorna** il CSV automaticamente. Usa la cella successiva per l'aggiornamento CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 24: ANALISI SINGOLO FILE (CON LOGGING)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Pipeline Multi-Agente - ModalitÃ  singolo file con logging dettagliato\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/single_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Aggiungi path progetto\n",
    "sys.path.insert(0, '/Users/danieledragoni/git/LIste')\n",
    "\n",
    "from app.agentic_pipeline import (\n",
    "    process_single_ptof,\n",
    "    AnalystAgent,\n",
    "    ReviewerAgent,\n",
    "    RefinerAgent,\n",
    "    SynthesizerAgent\n",
    ")\n",
    "\n",
    "logger.info(\"ğŸš€ Inizializzazione agenti multi-agente\")\n",
    "\n",
    "# Inizializza agenti\n",
    "analyst = AnalystAgent()\n",
    "reviewer = ReviewerAgent()\n",
    "refiner = RefinerAgent()\n",
    "synthesizer = SynthesizerAgent()\n",
    "\n",
    "logger.info(\"âœ… Agenti inizializzati con successo\")\n",
    "\n",
    "# Status callback con logging\n",
    "def status_callback(msg):\n",
    "    logger.info(f\"[PIPELINE] {msg}\")\n",
    "\n",
    "# Processa un singolo file PTOF\n",
    "md_file = \"ptof_md/MIIS08900V.md\"  # Modifica con il tuo file\n",
    "logger.info(f\"ğŸ“„ Processando file: {md_file}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "result = process_single_ptof(\n",
    "    md_file=md_file,\n",
    "    analyst=analyst,\n",
    "    reviewer=reviewer,\n",
    "    refiner=refiner,\n",
    "    synthesizer=synthesizer,\n",
    "    status_callback=status_callback\n",
    ")\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"âœ… Analisi completata in {elapsed:.1f} secondi!\")\n",
    "logger.info(f\"ğŸ“Š Risultato keys: {list(result.keys()) if result else 'None'}\")\n",
    "logger.info(f\"ğŸ“‹ Log salvato in: logs/single_analysis.log\")\n",
    "\n",
    "if result:\n",
    "\n",
    "    metadata = result.get('metadata', {})    logger.info(f\"ğŸ“ Comune: {metadata.get('comune', 'N/A')}\")\n",
    "    logger.info(f\"ğŸ« Scuola: {metadata.get('denominazione', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Analisi Singolo File (con CSV Automatico)\n",
    "\n",
    "Processa un singolo PTOF e **aggiorna automaticamente il CSV** per la dashboard.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Posizione | Descrizione |\n",
    "|-----------|-----------|-------------|\n",
    "| `md_file` | Nel codice | Path al file markdown da analizzare (es. `\"ptof_md/MIIS08900V.md\"`) |\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. **Verifica percorsi**: Controlla che tutte le directory esistano\n",
    "2. **Analisi multi-agente**: Analyst â†’ Reviewer â†’ Refiner â†’ Synthesizer\n",
    "3. **Verifica output**: Controlla che JSON e MD siano stati creati\n",
    "4. **Aggiorna CSV**: Rigenera `data/analysis_summary.csv`\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Per analizzare un PTOF specifico\n",
    "- Per testare la pipeline su un singolo file\n",
    "- Quando vuoi vedere i risultati subito nella dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 26: ANALISI SINGOLO FILE (CON CSV AUTOMATICO)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Pipeline Multi-Agente - Singolo File con AGGIORNAMENTO CSV AUTOMATICO\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/single_analysis_csv.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Aggiungi path progetto\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "from app.agentic_pipeline import (\n",
    "    process_single_ptof,\n",
    "    AnalystAgent,\n",
    "    ReviewerAgent,\n",
    "    RefinerAgent,\n",
    "    SynthesizerAgent\n",
    ")\n",
    "\n",
    "# Directory costanti (relative alla root del progetto)\n",
    "ANALYSIS_DIR = BASE_DIR / \"analysis_results\"\n",
    "MD_DIR = BASE_DIR / \"ptof_md\"\n",
    "CSV_FILE = BASE_DIR / \"data\" / \"analysis_summary.csv\"\n",
    "PTOF_PROCESSED_DIR = BASE_DIR / \"ptof_processed\"\n",
    "PTOF_INBOX_DIR = BASE_DIR / \"ptof_inbox\"\n",
    "\n",
    "def verify_paths_for_dashboard():\n",
    "    \"\"\"Verifica che tutti i percorsi siano accessibili dalla dashboard.\"\"\"\n",
    "    logger.info(\"ğŸ” Verifica percorsi per dashboard...\")\n",
    "    issues = []\n",
    "    \n",
    "    # Verifica directory esistono\n",
    "    for dir_path, name in [\n",
    "        (ANALYSIS_DIR, \"analysis_results\"),\n",
    "        (MD_DIR, \"ptof_md\"),\n",
    "        (PTOF_PROCESSED_DIR, \"ptof_processed\"),\n",
    "        (PTOF_INBOX_DIR, \"ptof_inbox\"),\n",
    "    ]:\n",
    "        if not dir_path.exists():\n",
    "            issues.append(f\"âŒ Directory mancante: {name}\")\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"  ğŸ“ Creata directory: {name}\")\n",
    "        else:\n",
    "            logger.info(f\"  âœ… {name}: OK\")\n",
    "    \n",
    "    # Verifica CSV esiste o puÃ² essere creato\n",
    "    if not CSV_FILE.parent.exists():\n",
    "        CSV_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not CSV_FILE.exists():\n",
    "        issues.append(f\"âš ï¸ CSV mancante: {CSV_FILE.name} (verrÃ  creato)\")\n",
    "    else:\n",
    "        logger.info(f\"  âœ… CSV: {CSV_FILE.name} OK\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def update_csv_after_analysis():\n",
    "    \"\"\"Aggiorna il CSV dopo l'analisi per la dashboard.\"\"\"\n",
    "    logger.info(\"ğŸ“Š Aggiornamento CSV per dashboard...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['python', 'src/processing/rebuild_csv.py'],\n",
    "            cwd=str(BASE_DIR),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=120\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            logger.info(\"  âœ… CSV aggiornato con successo!\")\n",
    "            # Conta righe\n",
    "            if CSV_FILE.exists():\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(CSV_FILE)\n",
    "                logger.info(f\"  ğŸ“ˆ Totale scuole nel CSV: {len(df)}\")\n",
    "        else:\n",
    "            logger.error(f\"  âŒ Errore rebuild CSV: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  âŒ Eccezione rebuild CSV: {e}\")\n",
    "\n",
    "def verify_analysis_files(school_code):\n",
    "    \"\"\"Verifica che i file dell'analisi siano accessibili.\"\"\"\n",
    "    logger.info(f\"ğŸ” Verifica file per {school_code}...\")\n",
    "    \n",
    "    files_to_check = [\n",
    "        (ANALYSIS_DIR / f\"{school_code}_analysis.json\", \"JSON analisi\"),\n",
    "        (ANALYSIS_DIR / f\"{school_code}_analysis.md\", \"MD analisi\"),\n",
    "        (MD_DIR / f\"{school_code}.md\", \"MD sorgente\"),\n",
    "    ]\n",
    "    \n",
    "    all_ok = True\n",
    "    for file_path, name in files_to_check:\n",
    "        if file_path.exists():\n",
    "            logger.info(f\"  âœ… {name}: {file_path.name}\")\n",
    "        else:\n",
    "            logger.warning(f\"  âš ï¸ {name}: MANCANTE\")\n",
    "            all_ok = False\n",
    "    \n",
    "    # Verifica PDF in processed o inbox\n",
    "    pdf_found = False\n",
    "    for pdf_dir in [PTOF_PROCESSED_DIR, PTOF_INBOX_DIR]:\n",
    "        for pdf in pdf_dir.rglob(f\"*{school_code}*.pdf\"):\n",
    "            logger.info(f\"  âœ… PDF: {pdf.relative_to(BASE_DIR)}\")\n",
    "            pdf_found = True\n",
    "            break\n",
    "        if pdf_found:\n",
    "            break\n",
    "    \n",
    "    if not pdf_found:\n",
    "        logger.warning(f\"  âš ï¸ PDF non trovato per {school_code}\")\n",
    "    \n",
    "    return all_ok\n",
    "\n",
    "# =====================================================\n",
    "# ESECUZIONE PRINCIPALE\n",
    "# =====================================================\n",
    "\n",
    "logger.info(\"ğŸš€ Pipeline Multi-Agente con Aggiornamento CSV\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# 1. Verifica percorsi\n",
    "path_issues = verify_paths_for_dashboard()\n",
    "if path_issues:\n",
    "    for issue in path_issues:\n",
    "        logger.warning(issue)\n",
    "\n",
    "# 2. Inizializza agenti\n",
    "logger.info(\"\\nğŸ¤– Inizializzazione agenti...\")\n",
    "analyst = AnalystAgent()\n",
    "reviewer = ReviewerAgent()\n",
    "refiner = RefinerAgent()\n",
    "synthesizer = SynthesizerAgent()\n",
    "logger.info(\"  âœ… Agenti pronti\")\n",
    "\n",
    "# Status callback con logging\n",
    "def status_callback(msg):\n",
    "    logger.info(f\"  [PIPELINE] {msg}\")\n",
    "\n",
    "# 3. Processa file PTOF\n",
    "md_file = \"ptof_md/ESEMPIO_CODICE.md\"  # ğŸ‘ˆ MODIFICA CON IL TUO FILE\n",
    "\n",
    "if not os.path.exists(md_file):\n",
    "    logger.error(f\"âŒ File non trovato: {md_file}\")\n",
    "    logger.info(\"ğŸ“‹ File MD disponibili:\")\n",
    "    for f in sorted(MD_DIR.glob(\"*.md\"))[:10]:\n",
    "        logger.info(f\"    - {f.name}\")\n",
    "else:\n",
    "    school_code = Path(md_file).stem\n",
    "    logger.info(f\"\\nğŸ“„ Processando: {md_file}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    result = process_single_ptof(\n",
    "        md_file=md_file,\n",
    "        analyst=analyst,\n",
    "        reviewer=reviewer,\n",
    "        refiner=refiner,\n",
    "        synthesizer=synthesizer,\n",
    "        status_callback=status_callback\n",
    "    )\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"âœ… Analisi completata in {elapsed:.1f} secondi!\")\n",
    "    \n",
    "    if result:\n",
    "        metadata = result.get('metadata', {})\n",
    "        logger.info(f\"ğŸ« Scuola: {metadata.get('denominazione', 'N/A')}\")\n",
    "        logger.info(f\"ğŸ“ Comune: {metadata.get('comune', 'N/A')}\")\n",
    "        logger.info(f\"ğŸ—ºï¸ Regione: {metadata.get('regione', 'N/A')}\")\n",
    "        logger.info(f\"ğŸ“® Provincia: {metadata.get('provincia', 'N/A')}\")\n",
    "    \n",
    "    # 4. Verifica file creati\n",
    "    verify_analysis_files(school_code)\n",
    "    \n",
    "    # 5. Aggiorna CSV per dashboard\n",
    "    update_csv_after_analysis()\n",
    "\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*60)logger.info(\"ğŸ“‹ Log: logs/single_analysis_csv.log\")\n",
    "logger.info(\"âœ… Pipeline completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§  Verifica Intelligente Percorsi (con LLM)\n",
    "\n",
    "Usa un LLM per analizzare la coerenza di tutti i file nel progetto.\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Posizione | Descrizione | Default |\n",
    "|-----------|-----------|-------------|---------|\n",
    "| `MODEL` | Nel codice | Modello Ollama per analisi | `qwen3:32b` |\n",
    "| `OLLAMA_URL` | Nel codice | URL server Ollama | `http://192.168.129.14:11434` |\n",
    "\n",
    "### ğŸ“‹ Cosa verifica\n",
    "\n",
    "| Check | Descrizione |\n",
    "|-------|-------------|\n",
    "| JSON â†’ MD | Ogni analisi ha il file markdown sorgente |\n",
    "| JSON â†’ PDF | Ogni analisi ha il PDF corrispondente |\n",
    "| Metadati | Campi obbligatori presenti (denominazione, regione, ecc.) |\n",
    "| school_id | Coerenza tra nome file e metadati |\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Per audit completo del progetto\n",
    "- Dopo import massivi di nuovi PTOF\n",
    "- Prima di archiviare risultati finali\n",
    "- Per diagnosticare problemi nella dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 28: VERIFICA INTELLIGENTE CON LLM\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Verifica Intelligente Percorsi con LLM\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BASE_DIR = Path('/Users/danieledragoni/git/LIste')\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "ANALYSIS_DIR = BASE_DIR / \"analysis_results\"\n",
    "MD_DIR = BASE_DIR / \"ptof_md\"\n",
    "PTOF_PROCESSED = BASE_DIR / \"ptof_processed\"\n",
    "PTOF_INBOX = BASE_DIR / \"ptof_inbox\"\n",
    "CSV_FILE = BASE_DIR / \"data\" / \"analysis_summary.csv\"\n",
    "\n",
    "# Configurazione Ollama\n",
    "OLLAMA_URL = \"http://192.168.129.14:11434/api/generate\"\n",
    "MODEL = \"qwen3:32b\"  # Modello veloce per verifiche\n",
    "\n",
    "def call_ollama(prompt, max_tokens=500):\n",
    "    \"\"\"Chiama Ollama per analisi intelligente.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json={\n",
    "            \"model\": MODEL,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0.1, \"num_predict\": max_tokens}\n",
    "        }, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Ollama non disponibile: {e}\")\n",
    "    return None\n",
    "\n",
    "def collect_file_inventory():\n",
    "    \"\"\"Raccoglie inventario di tutti i file.\"\"\"\n",
    "    inventory = {\n",
    "        'json_files': {},\n",
    "        'md_files': set(),\n",
    "        'pdf_files': set(),\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # JSON analisi\n",
    "    for json_path in ANALYSIS_DIR.glob(\"*_analysis.json\"):\n",
    "        code = json_path.stem.replace(\"_analysis\", \"\")\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            inventory['json_files'][code] = {\n",
    "                'path': json_path,\n",
    "                'metadata': data.get('metadata', {}),\n",
    "                'has_scores': 'ptof_section2' in data\n",
    "            }\n",
    "        except Exception as e:\n",
    "            inventory['issues'].append(f\"JSON corrotto: {json_path.name} - {e}\")\n",
    "    \n",
    "    # MD sorgente\n",
    "    for md_path in MD_DIR.glob(\"*.md\"):\n",
    "        inventory['md_files'].add(md_path.stem)\n",
    "    \n",
    "    # PDF (processed e inbox)\n",
    "    for pdf_dir in [PTOF_PROCESSED, PTOF_INBOX]:\n",
    "        for pdf in pdf_dir.rglob(\"*.pdf\"):\n",
    "            # Estrai codice dal nome file\n",
    "            name = pdf.stem.upper()\n",
    "            import re\n",
    "            match = re.search(r'([A-Z]{2,4}[A-Z0-9]{5,8}[A-Z0-9])', name)\n",
    "            if match:\n",
    "                inventory['pdf_files'].add(match.group(1))\n",
    "    \n",
    "    return inventory\n",
    "\n",
    "def verify_file_consistency(inventory):\n",
    "    \"\"\"Verifica coerenza tra file.\"\"\"\n",
    "    issues = list(inventory['issues'])\n",
    "    \n",
    "    for code, json_info in inventory['json_files'].items():\n",
    "        meta = json_info['metadata']\n",
    "        \n",
    "        # Verifica MD sorgente esiste\n",
    "        if code not in inventory['md_files']:\n",
    "            issues.append(f\"âš ï¸ {code}: MD sorgente mancante in ptof_md/\")\n",
    "        \n",
    "        # Verifica PDF esiste\n",
    "        if code.upper() not in inventory['pdf_files']:\n",
    "            issues.append(f\"âš ï¸ {code}: PDF non trovato in processed/inbox\")\n",
    "        \n",
    "        # Verifica metadati completi\n",
    "        required_fields = ['denominazione', 'regione', 'provincia', 'comune']\n",
    "        missing_meta = [f for f in required_fields if not meta.get(f) or meta.get(f) == 'ND']\n",
    "        if missing_meta:\n",
    "            issues.append(f\"âš ï¸ {code}: Metadati mancanti: {', '.join(missing_meta)}\")\n",
    "        \n",
    "        # Verifica coerenza school_id\n",
    "        if meta.get('school_id') and meta['school_id'].upper() != code.upper():\n",
    "            issues.append(f\"âŒ {code}: school_id nel JSON ({meta['school_id']}) non corrisponde al nome file\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def llm_analyze_issues(issues, sample_data):\n",
    "    \"\"\"Usa LLM per analisi intelligente dei problemi.\"\"\"\n",
    "    if not issues:\n",
    "        return \"Nessun problema rilevato. Tutti i file sono coerenti e raggiungibili dalla dashboard.\"\n",
    "    \n",
    "    prompt = f\"\"\"/no_think\n",
    "Sei un esperto di sistemi di analisi PTOF. Analizza questi problemi e suggerisci soluzioni:\n",
    "\n",
    "PROBLEMI RILEVATI ({len(issues)}):\n",
    "{chr(10).join(issues[:20])}\n",
    "\n",
    "CONTESTO:\n",
    "- Directory analysis_results/: contiene JSON analisi\n",
    "- Directory ptof_md/: contiene MD sorgenti\n",
    "- Directory ptof_processed/ e ptof_inbox/: contengono PDF\n",
    "- Dashboard Streamlit legge da data/analysis_summary.csv\n",
    "\n",
    "Fornisci:\n",
    "1. PrioritÃ  dei problemi (critico/medio/basso)\n",
    "2. Comandi o azioni per risolverli\n",
    "3. Suggerimenti per prevenire problemi futuri\n",
    "\n",
    "Rispondi in italiano, formato conciso.\"\"\"\n",
    "\n",
    "    response = call_ollama(prompt, max_tokens=800)\n",
    "    return response\n",
    "\n",
    "# =====================================================\n",
    "# ESECUZIONE VERIFICA\n",
    "# =====================================================\n",
    "\n",
    "logger.info(\"ğŸ” VERIFICA INTELLIGENTE PERCORSI FILE\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# 1. Raccogli inventario\n",
    "logger.info(\"\\nğŸ“‚ Raccolta inventario file...\")\n",
    "inventory = collect_file_inventory()\n",
    "logger.info(f\"  JSON analisi: {len(inventory['json_files'])}\")\n",
    "logger.info(f\"  MD sorgenti: {len(inventory['md_files'])}\")\n",
    "logger.info(f\"  PDF trovati: {len(inventory['pdf_files'])}\")\n",
    "\n",
    "# 2. Verifica coerenza\n",
    "logger.info(\"\\nğŸ” Verifica coerenza file...\")\n",
    "issues = verify_file_consistency(inventory)\n",
    "\n",
    "if issues:\n",
    "    logger.warning(f\"\\nâš ï¸ PROBLEMI RILEVATI: {len(issues)}\")\n",
    "    for issue in issues[:15]:  # Mostra max 15\n",
    "        logger.warning(f\"  {issue}\")\n",
    "    if len(issues) > 15:\n",
    "        logger.warning(f\"  ... e altri {len(issues) - 15} problemi\")\n",
    "else:\n",
    "    logger.info(\"  âœ… Nessun problema rilevato!\")\n",
    "\n",
    "# 3. Analisi LLM (se ci sono problemi)\n",
    "if issues:\n",
    "    logger.info(\"\\nğŸ¤– Analisi intelligente con LLM...\")\n",
    "    sample_data = list(inventory['json_files'].items())[:3]\n",
    "    llm_analysis = llm_analyze_issues(issues, sample_data)\n",
    "    if llm_analysis:\n",
    "        logger.info(\"\\nğŸ“‹ ANALISI LLM:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(llm_analysis)\n",
    "        print(\"-\" * 60)\n",
    "    else:\n",
    "        logger.info(\"  LLM non disponibile, verifica manuale consigliata\")\n",
    "\n",
    "# 4. Verifica CSV per dashboard\n",
    "logger.info(\"\\nğŸ“Š Verifica CSV dashboard...\")\n",
    "if CSV_FILE.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    logger.info(f\"  âœ… CSV esistente: {len(df)} scuole\")\n",
    "    \n",
    "    # Verifica colonne necessarie\n",
    "    required_cols = ['school_id', 'denominazione', 'regione', 'provincia', 'email', 'pec']\n",
    "    missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.warning(f\"  âš ï¸ Colonne mancanti nel CSV: {missing_cols}\")\n",
    "        logger.info(\"  ğŸ’¡ Esegui: python src/processing/rebuild_csv.py\")\n",
    "else:\n",
    "    logger.warning(\"  âŒ CSV non trovato!\")\n",
    "    logger.info(\"  ğŸ’¡ Esegui: python src/processing/rebuild_csv.py\")\n",
    "\n",
    "logger.info(\"âœ… Verifica completata!\")\n",
    "logger.info(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## â˜ï¸ Cloud Agent (Gemini/OpenRouter)\n",
    "\n",
    "Analisi PTOF usando API cloud invece di Ollama locale. Utile quando:\n",
    "- Ollama non Ã¨ disponibile\n",
    "- Vuoi usare modelli piÃ¹ potenti (GPT-4, Gemini Pro)\n",
    "- Hai limiti di RAM locale\n",
    "\n",
    "### âš™ï¸ Configurazione\n",
    "\n",
    "| Parametro | Posizione | Descrizione |\n",
    "|-----------|-----------|-------------|\n",
    "| `md_file` | Nel codice | File markdown da analizzare |\n",
    "| API Keys | `data/api_config.json` | Chiavi API per Gemini/OpenRouter |\n",
    "\n",
    "### ğŸ“‹ Configurazione API\n",
    "\n",
    "Crea/modifica `data/api_config.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"default_provider\": \"gemini\",\n",
    "  \"gemini_api_key\": \"YOUR_GEMINI_API_KEY\",\n",
    "  \"openrouter_api_key\": \"YOUR_OPENROUTER_API_KEY\"\n",
    "}\n",
    "```\n",
    "\n",
    "### âš ï¸ Note\n",
    "\n",
    "- Richiede connessione internet\n",
    "- Potrebbe avere costi (verifica limiti gratuiti)\n",
    "- Log salvati in `logs/cloud_analysis.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 30: CLOUD AGENT (GEMINI/OPENROUTER)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Cloud Agent - Analisi completa con logging dettagliato\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, '/Users/danieledragoni/git/LIste')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/cloud_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from src.processing.cloud_review import review_ptof_with_cloud, load_api_config\n",
    "\n",
    "logger.info(\"â˜ï¸ Inizializzazione Cloud Agent\")\n",
    "\n",
    "# Carica configurazione API dal file JSON\n",
    "config = load_api_config()\n",
    "provider = config.get('default_provider', 'gemini')\n",
    "api_key_field = f'{provider}_api_key'\n",
    "\n",
    "logger.info(f\"ğŸ” Provider: {provider}\")\n",
    "logger.info(f\"ğŸ”‘ API Key: {'âœ… Configurata' if config.get(api_key_field) else 'âŒ Mancante'}\")\n",
    "\n",
    "# Carica il contenuto del file MD\n",
    "md_file = 'ptof_md/MIIS08900V.md'\n",
    "logger.info(f\"ğŸ“„ Caricando file: {md_file}\")\n",
    "\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "logger.info(f\"ğŸ“ Dimensione documento: {len(text):,} caratteri\")\n",
    "\n",
    "# Esegui analisi completa\n",
    "start_time = datetime.now()\n",
    "logger.info(\"ğŸš€ Avvio analisi cloud...\")\n",
    "\n",
    "result = review_ptof_with_cloud(\n",
    "    md_content=text,\n",
    "    provider=provider,\n",
    "    api_key=config.get(api_key_field),\n",
    "    model=\"gemini-2.0-flash-exp\" if provider == 'gemini' else \"google/gemini-2.0-flash-exp:free\",\n",
    "    school_id=\"MIIS08900V\"\n",
    ")\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"âœ… Analisi completata in {elapsed:.1f} secondi!\")\n",
    "\n",
    "if result:\n",
    "    metadata = result.get('metadata', {})\n",
    "    logger.info(f\"ğŸ« School ID: {metadata.get('school_id', 'N/A')}\")\n",
    "    logger.info(f\"ğŸ“› Denominazione: {metadata.get('denominazione', 'N/A')}\")\n",
    "    logger.info(f\"ğŸ“ Comune: {metadata.get('comune', 'N/A')}\")\n",
    "\n",
    "    logger.info(f\"ğŸ—ºï¸ Area: {metadata.get('area_geografica', 'N/A')}\")logger.info(f\"ğŸ“‹ Log salvato in: logs/cloud_analysis.log\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Rebuild CSV\n",
    "\n",
    "Rigenera il file `data/analysis_summary.csv` leggendo tutti i JSON in `analysis_results/`.\n",
    "\n",
    "### ğŸš€ Quando usarla\n",
    "\n",
    "- Dopo analisi batch per sincronizzare il CSV\n",
    "- Quando la dashboard mostra dati non aggiornati\n",
    "- Dopo aver corretto manualmente dei JSON\n",
    "- Dopo aver usato le celle di correzione incongruenze\n",
    "\n",
    "### ğŸ“‹ Output\n",
    "\n",
    "- `data/analysis_summary.csv`: CSV per la dashboard\n",
    "- `logs/rebuild_csv.log`: Log dell'operazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 32: REBUILD CSV\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "%%bash\n",
    "# Rebuild CSV - Ricostruzione indice da file JSON\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"ğŸ“Š Ricostruzione CSV da file JSON\"\n",
    "echo \"ğŸ“‚ Input: analysis_results/*.json\"\n",
    "echo \"ğŸ“„ Output: data/analysis_summary.csv\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python src/processing/rebuild_csv.py 2>&1 | tee logs/rebuild_csv.log\n",
    "\n",
    "\n",
    "echo \"========================================\"echo \"ğŸ“‹ Log: logs/rebuild_csv.log\"\n",
    "echo \"âœ… CSV ricostruito!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”„ Conversione PDF â†’ Markdown\n",
    "\n",
    "Converte tutti i PDF in `ptof/` in file Markdown.\n",
    "\n",
    "### âš™ï¸ Directory\n",
    "\n",
    "| Input | Output |\n",
    "|-------|--------|\n",
    "| `ptof/*.pdf` | `ptof_md/*.md` |\n",
    "\n",
    "### ğŸ“‹ Note\n",
    "\n",
    "- Usa questa cella solo se hai PDF nella cartella `ptof/`\n",
    "- Per il workflow normale, i PDF vanno in `ptof_inbox/`\n",
    "- Log salvato in `logs/pdf_conversion.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 34: CONVERSIONE PDF â†’ MARKDOWN\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "%%bash\n",
    "# Conversione PDF â†’ Markdown\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"ğŸ”„ Conversione PDF â†’ Markdown\"\n",
    "echo \"ğŸ“‚ Input: ptof/*.pdf\"\n",
    "echo \"ğŸ“‚ Output: ptof_md/*.md\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python src/processing/convert_pdfs_to_md.py 2>&1 | tee logs/pdf_conversion.log\n",
    "\n",
    "\n",
    "echo \"========================================\"echo \"ğŸ“‹ Log: logs/pdf_conversion.log\"\n",
    "echo \"âœ… Conversione completata!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ› ï¸ Background Fixer\n",
    "\n",
    "Corregge automaticamente anomalie rilevate nei JSON delle analisi.\n",
    "\n",
    "### ğŸ“‹ Cosa fa\n",
    "\n",
    "1. Legge `data/review_flags.json` per le anomalie segnalate\n",
    "2. Usa Ollama (qwen2.5-coder:7b) per correggere i dati\n",
    "3. Aggiorna i JSON in `analysis_results/`\n",
    "\n",
    "### âš™ï¸ Prerequisiti\n",
    "\n",
    "- File `data/review_flags.json` con anomalie\n",
    "- Ollama in esecuzione con modello qwen2.5-coder:7b\n",
    "\n",
    "### ğŸ“‹ Output\n",
    "\n",
    "- JSON corretti in `analysis_results/`\n",
    "- Log in `logs/background_fixer.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 36: BACKGROUND FIXER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "%%bash\n",
    "# Background Fixer - Correzione automatica anomalie\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"ğŸ› ï¸ Background Fixer - Avvio\"\n",
    "echo \"ğŸ“‚ Input: data/review_flags.json\"\n",
    "echo \"ğŸ“‚ Output: analysis_results/\"\n",
    "echo \"ğŸ¤– Modello: qwen2.5-coder:7b (Ollama)\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python run_fixer.py 2>&1 | tee logs/background_fixer.log\n",
    "\n",
    "\n",
    "echo \"========================================\"echo \"ğŸ“‹ Log: logs/background_fixer.log\"\n",
    "echo \"âœ… Correzione completata!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ” Diagnostica Sistema\n",
    "\n",
    "Verifica lo stato completo del sistema di analisi PTOF.\n",
    "\n",
    "### ğŸ“‹ Cosa controlla\n",
    "\n",
    "| Check | Descrizione |\n",
    "|-------|-------------|\n",
    "| Anomalie | File con flag di revisione |\n",
    "| File JSON | Analisi completate |\n",
    "| File MD | Markdown convertiti |\n",
    "| PDF inbox | PDF in attesa |\n",
    "| API Config | Stato chiavi API |\n",
    "\n",
    "### ğŸ“‹ Output\n",
    "\n",
    "- Riepilogo a schermo\n",
    "- Log dettagliato in `logs/system_diagnostics.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Œ CELLA 38: DIAGNOSTICA SISTEMA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# System Status Check - Diagnostica completa con logging\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/system_diagnostics.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"ğŸ“Š DIAGNOSTICA SISTEMA PTOF ANALYSIS\")\n",
    "logger.info(f\"ğŸ• Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Check review flags\n",
    "logger.info(\"\\nğŸš© ANOMALIE RILEVATE\")\n",
    "if os.path.exists(\"data/review_flags.json\"):\n",
    "    with open(\"data/review_flags.json\", 'r') as f:\n",
    "        flags = json.load(f)\n",
    "    logger.info(f\"  File con anomalie: {len(flags)}\")\n",
    "else:\n",
    "    logger.info(\"  Nessun file di anomalie trovato\")\n",
    "\n",
    "# Count files\n",
    "logger.info(\"\\nğŸ“„ FILE SISTEMA\")\n",
    "json_files = glob(\"analysis_results/*.json\")\n",
    "logger.info(f\"  File analisi JSON: {len(json_files)}\")\n",
    "\n",
    "md_files = glob(\"ptof_md/*.md\")\n",
    "logger.info(f\"  File Markdown: {len(md_files)}\")\n",
    "\n",
    "pdf_inbox = glob(\"ptof_inbox/*.pdf\")\n",
    "logger.info(f\"  PDF in inbox: {len(pdf_inbox)}\")\n",
    "\n",
    "# API Configuration\n",
    "logger.info(\"\\nğŸ” CONFIGURAZIONE API\")\n",
    "if os.path.exists(\"data/api_config.json\"):\n",
    "    with open(\"data/api_config.json\", 'r') as f:\n",
    "        api_config = json.load(f)\n",
    "    logger.info(f\"  Default Provider: {api_config.get('default_provider', 'N/A')}\")\n",
    "    logger.info(f\"  Gemini API: {'âœ…' if api_config.get('gemini_api_key') else 'âŒ'}\")\n",
    "    logger.info(f\"  OpenRouter API: {'âœ…' if api_config.get('openrouter_api_key') else 'âŒ'}\")\n",
    "\n",
    "\n",
    "logger.info(\"\\n=\"*80)logger.info(\"ğŸ“‹ Log: logs/system_diagnostics.log\")\n",
    "logger.info(\"âœ… Diagnostica completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ Note\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- âœ… **Virtual Environment**: Tutti i comandi usano `.venv`\n",
    "- âœ… **Logging**: Ogni operazione salva log in `logs/`\n",
    "- âœ… **Workflow Automatico**: Usa `workflow_ptof.py` per processare nuovi PDF\n",
    "- âš ï¸ **API Keys**: Configurate in `data/api_config.json`\n",
    "- âš ï¸ **Ollama**: Deve essere in esecuzione per pipeline multi-agente\n",
    "\n",
    "### Directory\n",
    "\n",
    "- `ptof_inbox/` - PDF da analizzare\n",
    "- `ptof_processed/` - PDF archiviati\n",
    "- `ptof_md/` - Markdown generati\n",
    "- `analysis_results/` - JSON analisi\n",
    "- `logs/` - File di log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
