{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è CLI Examples - PTOF Analysis System\n",
    "\n",
    "Questo notebook contiene tutti gli snippet di codice per utilizzare gli script CLI del sistema di analisi PTOF.\n",
    "\n",
    "**‚öôÔ∏è Configurazione**: Tutti i comandi utilizzano il virtual environment `.venv` del progetto.\n",
    "\n",
    "**üìÇ Struttura Directory**:\n",
    "- `ptof_inbox/` ‚Üí PDF da analizzare (INSERISCI QUI)\n",
    "- `ptof_processed/` ‚Üí PDF archiviati automaticamente\n",
    "\n",
    "## üìë Indice\n",
    "1. [üöÄ **Workflow Automatico**](#workflow-automatico) ‚≠ê PRIORIT√Ä\n",
    "2. [üîó Analisi Multi-Agente Batch](#multi-agent-batch)\n",
    "3. [‚òÅÔ∏è Cloud Agent](#cloud-agent)\n",
    "4. [üîç Analisi e Revisione](#analysis-review)\n",
    "5. [üõ†Ô∏è Utility](#utilities)\n",
    "6. [ü§ñ Automazione Background](#background-automation)\n",
    "7. [üìä Diagnostica](#diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Workflow Automatico {#workflow-automatico}\n",
    "\n",
    "### ‚≠ê Pipeline Completa: Inbox ‚Üí Processed\n",
    "\n",
    "**Cosa fa**:\n",
    "1. üì• Legge PDF da `ptof_inbox/`\n",
    "2. üìù Converte PDF ‚Üí Markdown\n",
    "3. ü§ñ Analizza con pipeline multi-agente\n",
    "4. üì¶ Sposta PDF in `ptof_processed/batch_TIMESTAMP/`\n",
    "5. üìä Aggiorna CSV dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ WORKFLOW AUTOMATICO PTOF\n",
      "üì• Input: ptof_inbox/\n",
      "‚úÖ Output: ptof_processed/batch_TIMESTAMP/\n",
      "========================================\n",
      "2025-12-22 00:39:43,545 - INFO - üìÇ Directory verificata: ptof_inbox/\n",
      "2025-12-22 00:39:43,545 - INFO - üìÇ Directory verificata: ptof_processed/\n",
      "2025-12-22 00:39:43,545 - INFO - üìÇ Directory verificata: ptof_md/\n",
      "2025-12-22 00:39:43,545 - INFO - üìÇ Directory verificata: analysis_results/\n",
      "2025-12-22 00:39:43,545 - INFO - üìÇ Directory verificata: logs/\n",
      "2025-12-22 00:39:43,545 - INFO - ================================================================================\n",
      "2025-12-22 00:39:43,545 - INFO - üöÄ AVVIO WORKFLOW PTOF COMPLETO\n",
      "2025-12-22 00:39:43,545 - INFO - üïê Timestamp: 2025-12-22 00:39:43\n",
      "2025-12-22 00:39:43,545 - INFO - ================================================================================\n",
      "2025-12-22 00:39:43,546 - INFO - \n",
      "üìä STATO INIZIALE:\n",
      "2025-12-22 00:39:43,546 - INFO -   PDF in inbox: 1\n",
      "2025-12-22 00:39:43,546 - INFO -   PDF processati: 0\n",
      "2025-12-22 00:39:43,546 - INFO -   File Markdown: 0\n",
      "2025-12-22 00:39:43,547 - INFO -   File analisi: 0\n",
      "2025-12-22 00:39:43,547 - INFO - ================================================================================\n",
      "2025-12-22 00:39:43,547 - INFO - üìù STEP 1: Conversione PDF ‚Üí Markdown\n",
      "2025-12-22 00:39:43,547 - INFO - üìÑ Trovati 1 PDF da convertire\n",
      "2025-12-22 00:39:44,291 - INFO - ‚úÖ Convertito: AGPC010001_PTOF\n",
      "2025-12-22 00:39:44,291 - INFO - üìä Convertiti 1/1 file\n",
      "2025-12-22 00:39:44,291 - INFO - ================================================================================\n",
      "2025-12-22 00:39:44,291 - INFO - ü§ñ STEP 2: Analisi Multi-Agente\n",
      "2025-12-22 00:39:47,105 - INFO - üìÑ File da analizzare: 1\n",
      "2025-12-22 00:39:47,105 - INFO - üîÑ Processando: AGPC010001_PTOF\n",
      "2025-12-22 00:39:47,105 - INFO -   [PIPELINE] Processing AGPC010001_PTOF...\n",
      "2025-12-22 00:39:47,106 - INFO - [Pipeline] Long document (495796 chars) - using chunked analysis\n",
      "2025-12-22 00:39:47,106 - INFO -   [PIPELINE] Long doc (495k chars) - chunking...\n",
      "2025-12-22 00:39:47,121 - INFO - [Pipeline] Split into 14 chunks\n",
      "2025-12-22 00:39:47,121 - INFO -   [PIPELINE] Analyst: Chunk 1/14...\n",
      "2025-12-22 00:39:47,121 - INFO - [gemma3:27b] Drafting chunk 1/14...\n",
      "2025-12-22 00:40:45,167 - INFO -   [PIPELINE] Analyst: Chunk 2/14...\n",
      "2025-12-22 00:40:45,168 - INFO - [gemma3:27b] Drafting chunk 2/14...\n",
      "2025-12-22 00:41:17,777 - INFO -   [PIPELINE] Analyst: Chunk 3/14...\n",
      "2025-12-22 00:41:17,777 - INFO - [gemma3:27b] Drafting chunk 3/14...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# WORKFLOW COMPLETO - Inbox ‚Üí Processed\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"üöÄ WORKFLOW AUTOMATICO PTOF\"\n",
    "echo \"üì• Input: ptof_inbox/\"\n",
    "echo \"‚úÖ Output: ptof_processed/batch_TIMESTAMP/\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python workflow_ptof.py 2>&1 | tee logs/workflow_ptof.log\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"‚úÖ Workflow completato!\"\n",
    "echo \"üìã Log: logs/workflow_ptof.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step Post-Analisi: Scova incongruenze nei nomi\n",
    "\n",
    "Esegui questo step **dopo** il workflow (PDF ‚Üí MD ‚Üí Analisi).\n",
    "Individua file con nomi non coerenti rispetto a `metadata.school_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "ANALYSIS_DIR = \"analysis_results\"\n",
    "MD_DIR = \"ptof_md\"\n",
    "\n",
    "def normalize_school_id(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "def is_valid_school_id(value):\n",
    "    value = normalize_school_id(value)\n",
    "    if not value or value.upper() in {\"ND\", \"N/A\", \"NONE\", \"UNKNOWN\"}:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def build_issues():\n",
    "    issues = []\n",
    "    json_files = glob.glob(os.path.join(ANALYSIS_DIR, \"*_analysis.json\"))\n",
    "    for json_path in sorted(json_files):\n",
    "        filename = os.path.basename(json_path)\n",
    "        base = filename.replace(\"_analysis.json\", \"\")\n",
    "        try:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            issues.append({\n",
    "                \"kind\": \"json_read_error\",\n",
    "                \"current\": json_path,\n",
    "                \"target\": \"\",\n",
    "                \"reason\": f\"json read error: {e}\",\n",
    "            })\n",
    "            continue\n",
    "        school_id = data.get(\"metadata\", {}).get(\"school_id\")\n",
    "        if not is_valid_school_id(school_id):\n",
    "            issues.append({\n",
    "                \"kind\": \"invalid_school_id\",\n",
    "                \"current\": json_path,\n",
    "                \"target\": \"\",\n",
    "                \"reason\": f\"metadata.school_id non valido: {school_id}\",\n",
    "            })\n",
    "            continue\n",
    "        school_id = normalize_school_id(school_id)\n",
    "        if base != school_id:\n",
    "            target_json = os.path.join(ANALYSIS_DIR, f\"{school_id}_analysis.json\")\n",
    "            if os.path.exists(target_json):\n",
    "                issues.append({\n",
    "                    \"kind\": \"conflict_json\",\n",
    "                    \"current\": json_path,\n",
    "                    \"target\": target_json,\n",
    "                    \"reason\": f\"dest esiste per {school_id}\",\n",
    "                })\n",
    "            else:\n",
    "                issues.append({\n",
    "                    \"kind\": \"rename_json\",\n",
    "                    \"current\": json_path,\n",
    "                    \"target\": target_json,\n",
    "                    \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                })\n",
    "\n",
    "            old_analysis_md = os.path.join(ANALYSIS_DIR, f\"{base}_analysis.md\")\n",
    "            new_analysis_md = os.path.join(ANALYSIS_DIR, f\"{school_id}_analysis.md\")\n",
    "            if os.path.exists(old_analysis_md):\n",
    "                if os.path.exists(new_analysis_md):\n",
    "                    issues.append({\n",
    "                        \"kind\": \"conflict_analysis_md\",\n",
    "                        \"current\": old_analysis_md,\n",
    "                        \"target\": new_analysis_md,\n",
    "                        \"reason\": f\"dest esiste per {school_id}\",\n",
    "                    })\n",
    "                else:\n",
    "                    issues.append({\n",
    "                        \"kind\": \"rename_analysis_md\",\n",
    "                        \"current\": old_analysis_md,\n",
    "                        \"target\": new_analysis_md,\n",
    "                        \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                    })\n",
    "\n",
    "            old_md = os.path.join(MD_DIR, f\"{base}.md\")\n",
    "            new_md = os.path.join(MD_DIR, f\"{school_id}.md\")\n",
    "            if os.path.exists(old_md):\n",
    "                if os.path.exists(new_md):\n",
    "                    issues.append({\n",
    "                        \"kind\": \"conflict_md\",\n",
    "                        \"current\": old_md,\n",
    "                        \"target\": new_md,\n",
    "                        \"reason\": f\"dest esiste per {school_id}\",\n",
    "                    })\n",
    "                else:\n",
    "                    issues.append({\n",
    "                        \"kind\": \"rename_md\",\n",
    "                        \"current\": old_md,\n",
    "                        \"target\": new_md,\n",
    "                        \"reason\": f\"nome base {base} != metadata.school_id\",\n",
    "                    })\n",
    "    return issues\n",
    "\n",
    "issues = build_issues()\n",
    "print(f\"Totale incongruenze trovate: {len(issues)}\")\n",
    "for idx, issue in enumerate(issues, 1):\n",
    "    target = issue.get(\"target\") or \"-\"\n",
    "    print(f\"[{idx:02d}] {issue['kind']} | {os.path.basename(issue['current'])} -> {os.path.basename(target)} | {issue['reason']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step Post-Analisi: Correggi incongruenze selezionate\n",
    "\n",
    "**Piano di correzione**\n",
    "1. Mostra il piano e richiede quali ID correggere.\n",
    "2. Rinomina solo le voci selezionate (senza sovrascrivere).\n",
    "3. Stampa un riepilogo finale.\n",
    "\n",
    "Imposta `dry_run = True` per una simulazione senza rinominare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_rename(src, dst, dry_run=False):\n",
    "    if os.path.abspath(src) == os.path.abspath(dst):\n",
    "        return False\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"‚ö†Ô∏è Skip rename (dest exists): {os.path.basename(dst)}\")\n",
    "        return False\n",
    "    if dry_run:\n",
    "        print(f\"[DRY] {os.path.basename(src)} -> {os.path.basename(dst)}\")\n",
    "        return True\n",
    "    os.rename(src, dst)\n",
    "    print(f\"‚úÖ Renamed: {os.path.basename(src)} -> {os.path.basename(dst)}\")\n",
    "    return True\n",
    "\n",
    "if not issues:\n",
    "    print(\"Nessuna incongruenza da correggere.\")\n",
    "else:\n",
    "    print(\"Piano: 1) selezione ID  2) rinomina  3) riepilogo\")\n",
    "    selection = input(\"Quali ID vuoi correggere? (es. 1,3,5 | all | none): \").strip().lower()\n",
    "    if selection in {\"\", \"none\", \"no\"}:\n",
    "        print(\"Nessuna correzione eseguita.\")\n",
    "    else:\n",
    "        if selection == \"all\":\n",
    "            ids = list(range(1, len(issues) + 1))\n",
    "        else:\n",
    "            ids = []\n",
    "            for part in selection.split(','):\n",
    "                part = part.strip()\n",
    "                if part.isdigit():\n",
    "                    ids.append(int(part))\n",
    "        dry_run = True\n",
    "        renamed = 0\n",
    "        for idx in ids:\n",
    "            if idx < 1 or idx > len(issues):\n",
    "                continue\n",
    "            issue = issues[idx - 1]\n",
    "            if not issue['kind'].startswith('rename_'):\n",
    "                print(f\"- Skip {idx:02d} ({issue['kind']})\")\n",
    "                continue\n",
    "            if safe_rename(issue['current'], issue['target'], dry_run=dry_run):\n",
    "                renamed += 1\n",
    "        print(f\"Rinominati: {renamed} (dry_run={dry_run})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica Stato Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "\n",
    "echo \"üìä STATO DIRECTORY\"\n",
    "echo \"========================================\"\n",
    "echo \"üì• Inbox PDF: $(find ptof_inbox -name '*.pdf' 2>/dev/null | wc -l)\"\n",
    "echo \"‚úÖ Processed PDF: $(find ptof_processed -name '*.pdf' 2>/dev/null | wc -l)\"\n",
    "echo \"üìù Markdown: $(find ptof_md -name '*.md' 2>/dev/null | wc -l)\"\n",
    "echo \"üìä Analisi JSON: $(find analysis_results -name '*.json' 2>/dev/null | wc -l)\"\n",
    "echo \"========================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó Analisi Multi-Agente Batch {#multi-agent-batch}\n",
    "\n",
    "### Pipeline Multi-Agente per Cartella Completa\n",
    "Processa tutti i file PTOF in una cartella usando l'architettura multi-agente (Analyst ‚Üí Reviewer ‚Üí Refiner ‚Üí Synthesizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Pipeline Multi-Agente - Batch completo su cartella ptof_md/\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"üöÄ Avvio Pipeline Multi-Agente Batch\"\n",
    "echo \"üìÇ Directory: ptof_md/\"\n",
    "echo \"ü§ñ Agenti: Analyst ‚Üí Reviewer ‚Üí Refiner\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python app/agentic_pipeline.py 2>&1 | tee logs/agentic_pipeline.log\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"‚úÖ Pipeline completata!\"\n",
    "echo \"üìã Log: logs/agentic_pipeline.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Multi-Agente - Singolo File con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Multi-Agente - Modalit√† singolo file con logging dettagliato\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/single_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Aggiungi path progetto\n",
    "sys.path.insert(0, '/Users/danieledragoni/git/LIste')\n",
    "\n",
    "from app.agentic_pipeline import (\n",
    "    process_single_ptof,\n",
    "    AnalystAgent,\n",
    "    ReviewerAgent,\n",
    "    RefinerAgent,\n",
    "    SynthesizerAgent\n",
    ")\n",
    "\n",
    "logger.info(\"üöÄ Inizializzazione agenti multi-agente\")\n",
    "\n",
    "# Inizializza agenti\n",
    "analyst = AnalystAgent()\n",
    "reviewer = ReviewerAgent()\n",
    "refiner = RefinerAgent()\n",
    "synthesizer = SynthesizerAgent()\n",
    "\n",
    "logger.info(\"‚úÖ Agenti inizializzati con successo\")\n",
    "\n",
    "# Status callback con logging\n",
    "def status_callback(msg):\n",
    "    logger.info(f\"[PIPELINE] {msg}\")\n",
    "\n",
    "# Processa un singolo file PTOF\n",
    "md_file = \"ptof_md/MIIS08900V.md\"  # Modifica con il tuo file\n",
    "logger.info(f\"üìÑ Processando file: {md_file}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "result = process_single_ptof(\n",
    "    md_file=md_file,\n",
    "    analyst=analyst,\n",
    "    reviewer=reviewer,\n",
    "    refiner=refiner,\n",
    "    synthesizer=synthesizer,\n",
    "    status_callback=status_callback\n",
    ")\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"‚úÖ Analisi completata in {elapsed:.1f} secondi!\")\n",
    "logger.info(f\"üìä Risultato keys: {list(result.keys()) if result else 'None'}\")\n",
    "logger.info(f\"üìã Log salvato in: logs/single_analysis.log\")\n",
    "\n",
    "if result:\n",
    "    metadata = result.get('metadata', {})\n",
    "    logger.info(f\"üè´ Scuola: {metadata.get('denominazione', 'N/A')}\")\n",
    "    logger.info(f\"üìç Comune: {metadata.get('comune', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚òÅÔ∏è Cloud Agent {#cloud-agent}\n",
    "\n",
    "### Analisi Cloud con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Agent - Analisi completa con logging dettagliato\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, '/Users/danieledragoni/git/LIste')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/cloud_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from src.processing.cloud_review import review_ptof_with_cloud, load_api_config\n",
    "\n",
    "logger.info(\"‚òÅÔ∏è Inizializzazione Cloud Agent\")\n",
    "\n",
    "# Carica configurazione API dal file JSON\n",
    "config = load_api_config()\n",
    "provider = config.get('default_provider', 'gemini')\n",
    "api_key_field = f'{provider}_api_key'\n",
    "\n",
    "logger.info(f\"üîê Provider: {provider}\")\n",
    "logger.info(f\"üîë API Key: {'‚úÖ Configurata' if config.get(api_key_field) else '‚ùå Mancante'}\")\n",
    "\n",
    "# Carica il contenuto del file MD\n",
    "md_file = 'ptof_md/MIIS08900V.md'\n",
    "logger.info(f\"üìÑ Caricando file: {md_file}\")\n",
    "\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "logger.info(f\"üìè Dimensione documento: {len(text):,} caratteri\")\n",
    "\n",
    "# Esegui analisi completa\n",
    "start_time = datetime.now()\n",
    "logger.info(\"üöÄ Avvio analisi cloud...\")\n",
    "\n",
    "result = review_ptof_with_cloud(\n",
    "    md_content=text,\n",
    "    provider=provider,\n",
    "    api_key=config.get(api_key_field),\n",
    "    model=\"gemini-2.0-flash-exp\" if provider == 'gemini' else \"google/gemini-2.0-flash-exp:free\",\n",
    "    school_id=\"MIIS08900V\"\n",
    ")\n",
    "\n",
    "elapsed = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"‚úÖ Analisi completata in {elapsed:.1f} secondi!\")\n",
    "\n",
    "if result:\n",
    "    metadata = result.get('metadata', {})\n",
    "    logger.info(f\"üè´ School ID: {metadata.get('school_id', 'N/A')}\")\n",
    "    logger.info(f\"üìõ Denominazione: {metadata.get('denominazione', 'N/A')}\")\n",
    "    logger.info(f\"üìç Comune: {metadata.get('comune', 'N/A')}\")\n",
    "    logger.info(f\"üó∫Ô∏è Area: {metadata.get('area_geografica', 'N/A')}\")\n",
    "    \n",
    "logger.info(f\"üìã Log salvato in: logs/cloud_analysis.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Analisi e Revisione {#analysis-review}\n",
    "\n",
    "### Rebuild CSV con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Rebuild CSV - Ricostruzione indice da file JSON\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"üìä Ricostruzione CSV da file JSON\"\n",
    "echo \"üìÇ Input: analysis_results/*.json\"\n",
    "echo \"üìÑ Output: data/analysis_summary.csv\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python src/processing/rebuild_csv.py 2>&1 | tee logs/rebuild_csv.log\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"‚úÖ CSV ricostruito!\"\n",
    "echo \"üìã Log: logs/rebuild_csv.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ†Ô∏è Utility {#utilities}\n",
    "\n",
    "### Conversione PDF ‚Üí Markdown con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Conversione PDF ‚Üí Markdown\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"üîÑ Conversione PDF ‚Üí Markdown\"\n",
    "echo \"üìÇ Input: ptof/*.pdf\"\n",
    "echo \"üìÇ Output: ptof_md/*.md\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python src/processing/convert_pdfs_to_md.py 2>&1 | tee logs/pdf_conversion.log\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"‚úÖ Conversione completata!\"\n",
    "echo \"üìã Log: logs/pdf_conversion.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Automazione Background {#background-automation}\n",
    "\n",
    "### Background Fixer - CLI con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Background Fixer - Correzione automatica anomalie\n",
    "cd /Users/danieledragoni/git/LIste\n",
    "source .venv/bin/activate\n",
    "\n",
    "echo \"üõ†Ô∏è Background Fixer - Avvio\"\n",
    "echo \"üìÇ Input: data/review_flags.json\"\n",
    "echo \"üìÇ Output: analysis_results/\"\n",
    "echo \"ü§ñ Modello: qwen2.5-coder:7b (Ollama)\"\n",
    "echo \"========================================\"\n",
    "\n",
    "python run_fixer.py 2>&1 | tee logs/background_fixer.log\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"‚úÖ Correzione completata!\"\n",
    "echo \"üìã Log: logs/background_fixer.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Diagnostica {#diagnostics}\n",
    "\n",
    "### Verifica Stato Sistema Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Status Check - Diagnostica completa con logging\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/system_diagnostics.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"üìä DIAGNOSTICA SISTEMA PTOF ANALYSIS\")\n",
    "logger.info(f\"üïê Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Check review flags\n",
    "logger.info(\"\\nüö© ANOMALIE RILEVATE\")\n",
    "if os.path.exists(\"data/review_flags.json\"):\n",
    "    with open(\"data/review_flags.json\", 'r') as f:\n",
    "        flags = json.load(f)\n",
    "    logger.info(f\"  File con anomalie: {len(flags)}\")\n",
    "else:\n",
    "    logger.info(\"  Nessun file di anomalie trovato\")\n",
    "\n",
    "# Count files\n",
    "logger.info(\"\\nüìÑ FILE SISTEMA\")\n",
    "json_files = glob(\"analysis_results/*.json\")\n",
    "logger.info(f\"  File analisi JSON: {len(json_files)}\")\n",
    "\n",
    "md_files = glob(\"ptof_md/*.md\")\n",
    "logger.info(f\"  File Markdown: {len(md_files)}\")\n",
    "\n",
    "pdf_inbox = glob(\"ptof_inbox/*.pdf\")\n",
    "logger.info(f\"  PDF in inbox: {len(pdf_inbox)}\")\n",
    "\n",
    "# API Configuration\n",
    "logger.info(\"\\nüîê CONFIGURAZIONE API\")\n",
    "if os.path.exists(\"data/api_config.json\"):\n",
    "    with open(\"data/api_config.json\", 'r') as f:\n",
    "        api_config = json.load(f)\n",
    "    logger.info(f\"  Default Provider: {api_config.get('default_provider', 'N/A')}\")\n",
    "    logger.info(f\"  Gemini API: {'‚úÖ' if api_config.get('gemini_api_key') else '‚ùå'}\")\n",
    "    logger.info(f\"  OpenRouter API: {'‚úÖ' if api_config.get('openrouter_api_key') else '‚ùå'}\")\n",
    "\n",
    "logger.info(\"\\n=\"*80)\n",
    "logger.info(\"‚úÖ Diagnostica completata!\")\n",
    "logger.info(\"üìã Log: logs/system_diagnostics.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Note\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- ‚úÖ **Virtual Environment**: Tutti i comandi usano `.venv`\n",
    "- ‚úÖ **Logging**: Ogni operazione salva log in `logs/`\n",
    "- ‚úÖ **Workflow Automatico**: Usa `workflow_ptof.py` per processare nuovi PDF\n",
    "- ‚ö†Ô∏è **API Keys**: Configurate in `data/api_config.json`\n",
    "- ‚ö†Ô∏è **Ollama**: Deve essere in esecuzione per pipeline multi-agente\n",
    "\n",
    "### Directory\n",
    "\n",
    "- `ptof_inbox/` - PDF da analizzare\n",
    "- `ptof_processed/` - PDF archiviati\n",
    "- `ptof_md/` - Markdown generati\n",
    "- `analysis_results/` - JSON analisi\n",
    "- `logs/` - File di log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
